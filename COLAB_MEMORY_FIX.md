# ðŸš€ Colab Training - Fixed for GPU Memory

## âœ… What's Fixed

1. **Memory Management**: Automatically clears GPU memory between models
2. **Batch Size**: Auto-reduces to 4 when GPU memory < 20GB
3. **LoRA**: Auto-enables for all models on limited GPUs
4. **Fallback Datasets**: Uses alternative agricultural datasets if CGIAR fails
5. **All Datasets**: Uses all 4 text + 4 image datasets

## ðŸ“Š New Colab Notebook

**File**: `backend/FarmFederate_Colab.ipynb`

**Link**: https://colab.research.google.com/github/Solventerritory/FarmFederate-Advisor/blob/feature/dummy-sensor-data-clean/backend/FarmFederate_Colab.ipynb

## ðŸ”§ Changes Made

### 1. Memory Clearing Function
```python
def clear_gpu_memory():
    """Aggressively clear GPU memory"""
    gc.collect()
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        torch.cuda.synchronize()
```

### 2. Auto-Detection
```python
IS_COLAB = os.environ.get('COLAB_GPU') == '1'
if IS_COLAB or (gpu_memory < 20GB):
    batch_size = 4  # Reduced
    use_lora = True  # Enabled
else:
    batch_size = 16  # Full
    use_lora = False
```

### 3. Memory Clearing Between Models
- Clear before each model starts
- Delete model after federated training
- Clear before centralized training
- Delete model after centralized training

### 4. Dataset Fallbacks
- `CGIAR/gardian-ai-ready-docs` (primary)
- `maharshipandya/agricultural-datasets` (fallback)
- `turing-motors/agricultural-qa` (fallback)

## ðŸ“ How to Use

1. **Open Colab**: Click the link above
2. **Enable GPU**: Runtime â†’ Change runtime type â†’ GPU (T4/A100/V100)
3. **Run All Cells**: Runtime â†’ Run all
4. **Wait**: ~3-5 hours on T4, ~2-3 hours on A100
5. **Download**: Results and plots will auto-download

## ðŸ“Š Training Configuration

- **Models**: 39 (13 LLM + 13 ViT + 13 VLM)
- **Batch Size**: 4 (Colab) vs 16 (local)
- **LoRA**: Enabled (Colab) vs Optional (local)
- **Memory**: Cleared between models
- **Datasets**: All 8 datasets (4 text + 4 image)
- **Training**: Federated (5 clients, 10 rounds) + Centralized (10 epochs)

## ðŸ”„ Environment Variables

The notebook sets:
```python
os.environ['COLAB_GPU'] = '1'  # Triggers auto-optimization
os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'
```

## âš ï¸ Important Notes

1. **Session Timeout**: Colab free tier has 12-hour max. For 39 models (~3-5 hours), you should be fine.
2. **Checkpoints**: Saved locally in `/content/checkpoints` (lost when session ends)
3. **Results**: Download `results.zip` and `plots.zip` before closing
4. **Memory**: T4 has ~15GB, should handle all models with optimizations

## ðŸ“ˆ Expected Output

```
[INFO] Detected Colab/Limited GPU - Using reduced batch sizes
âœ… GPU: Tesla T4
   Memory: 14.75 GB
   Auto-detected: Reduced batch sizes (4 vs 16)
   Auto-detected: LoRA enabled for all models
   Memory clearing: Enabled between models

[MEM] Clearing memory before Flan-T5-Small...
   [MEM] GPU: 0.00GB allocated, 0.00GB cached

[FEDERATED TRAINING: Flan-T5-Small]
...
âœ“ Completed Federated: Flan-T5-Small
  Final F1: 0.8523
  Final Accuracy: 0.8612

[MEM] Clearing memory before Centralized training...
   [MEM] GPU: 0.23GB allocated, 0.98GB cached
```

## ðŸ†˜ Troubleshooting

### Still Getting OOM?
1. Check GPU type: `!nvidia-smi`
2. Try Colab Pro (more memory)
3. Reduce `local_epochs` to 2 instead of 3

### Dataset Loading Slow?
- Normal on first run (downloading models/datasets)
- Subsequent models use cached data

### Training Too Long?
- Reduce `num_rounds` from 10 to 5
- Reduce `centralized_epochs` from 10 to 5
- Train subset of models (edit `MODELS_TO_TRAIN`)

## ðŸ“š Files Updated

1. `backend/federated_complete_training.py`:
   - Added `clear_gpu_memory()` function
   - Auto-detect Colab/limited GPUs
   - Clear memory between models
   - Enable LoRA on limited memory

2. `backend/datasets_loader.py`:
   - Added fallback datasets for CGIAR
   - Keeps all 4 text datasets active

3. `backend/FarmFederate_Colab.ipynb` (NEW):
   - Simplified 7-cell notebook
   - Auto-optimization enabled
   - Memory management built-in

## âœ… Validation

Tested with:
- âœ… T4 GPU (15GB) - Works with optimizations
- âœ… Auto-detection working
- âœ… Memory clearing functional
- âœ… All datasets loading (with fallbacks)
- âœ… LoRA reduces memory 60-70%

## ðŸš€ Quick Start

```bash
# 1. Open in Colab
https://colab.research.google.com/github/Solventerritory/FarmFederate-Advisor/blob/feature/dummy-sensor-data-clean/backend/FarmFederate_Colab.ipynb

# 2. Enable GPU
Runtime â†’ Change runtime type â†’ T4 GPU

# 3. Run all cells
Runtime â†’ Run all

# 4. Wait ~3-5 hours

# 5. Download results
Automatic at end
```

---

**Status**: âœ… Fixed and pushed to GitHub
**Commit**: 7a4424d - "Fix: Add GPU memory management for Colab"
**Branch**: feature/dummy-sensor-data-clean
