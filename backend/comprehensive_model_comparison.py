"""
Comprehensive Model Comparison Framework
=========================================

Compares:
1. Inter-category: LLM vs ViT vs VLM
2. Intra-category: Within each model type
3. Learning paradigm: Centralized vs Federated for each model

Author: FarmFederate Research Team
Date: 2026-01-15
"""

import os
import json
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib.patches as mpatches
from matplotlib.gridspec import GridSpec
import seaborn as sns
from scipy import stats
from typing import Dict, List, Tuple
import warnings

warnings.filterwarnings('ignore')

# Publication-quality settings
plt.rcParams.update({
    'figure.dpi': 300,
    'savefig.dpi': 300,
    'font.family': 'sans-serif',
    'font.size': 10,
    'axes.labelsize': 11,
    'axes.titlesize': 12,
    'xtick.labelsize': 9,
    'ytick.labelsize': 9,
    'legend.fontsize': 9,
})

# Color scheme
IEEE_COLORS = {
    'blue': '#0C5DA5', 'orange': '#FF9500', 'green': '#00B945',
    'red': '#FF2C00', 'purple': '#845B97', 'brown': '#965C46',
    'pink': '#F97BB4', 'gray': '#474747', 'olive': '#9A8B3A', 'cyan': '#00B8C5'
}

# Model configurations
LLM_MODELS = {
    'flan-t5-small': {'params': 80, 'type': 'encoder-decoder'},
    'flan-t5-base': {'params': 250, 'type': 'encoder-decoder'},
    't5-small': {'params': 60, 'type': 'encoder-decoder'},
    'gpt2': {'params': 124, 'type': 'decoder-only'},
    'gpt2-medium': {'params': 355, 'type': 'decoder-only'},
    'distilgpt2': {'params': 82, 'type': 'decoder-only'},
    'roberta-base': {'params': 125, 'type': 'encoder-only'},
    'bert-base': {'params': 110, 'type': 'encoder-only'},
    'distilbert': {'params': 66, 'type': 'encoder-only'},
}

VIT_MODELS = {
    'vit-base-224': {'params': 86, 'patch_size': 16, 'image_size': 224},
    'vit-large-224': {'params': 304, 'patch_size': 16, 'image_size': 224},
    'vit-base-384': {'params': 86, 'patch_size': 16, 'image_size': 384},
    'deit-base': {'params': 86, 'patch_size': 16, 'image_size': 224},
}

VLM_MODELS = {
    'clip-vit-base': {'params': 151, 'type': 'contrastive'},
    'clip-vit-large': {'params': 428, 'type': 'contrastive'},
    'blip-base': {'params': 224, 'type': 'generative'},
    'blip2-opt': {'params': 2700, 'type': 'generative'},
}


# ============================================================================
# Section 1: Data Generation (Simulated Results)
# ============================================================================

def generate_comprehensive_results():
    """
    Generate comprehensive results for all models in both centralized and federated settings.
    In production, replace with actual training results.
    """
    np.random.seed(42)
    results = {
        'llm': {'centralized': {}, 'federated': {}},
        'vit': {'centralized': {}, 'federated': {}},
        'vlm': {'centralized': {}, 'federated': {}},
    }

    # LLM Models (text-based, lower performance)
    for model_name, config in LLM_MODELS.items():
        base_f1 = 0.70 + np.random.uniform(0, 0.10)

        # Centralized: better performance
        results['llm']['centralized'][model_name] = {
            'f1_macro': base_f1 + 0.15,
            'f1_micro': base_f1 + 0.16,
            'accuracy': base_f1 + 0.17,
            'precision': base_f1 + 0.14,
            'recall': base_f1 + 0.15,
            'training_time': config['params'] * 0.05,  # minutes
            'communication_cost': 0,  # No communication in centralized
            'convergence_rounds': 1,
            'per_class': [base_f1 + 0.15 + np.random.uniform(-0.05, 0.05) for _ in range(5)]
        }

        # Federated: privacy-preserving, slightly lower performance
        results['llm']['federated'][model_name] = {
            'f1_macro': base_f1,
            'f1_micro': base_f1 + 0.01,
            'accuracy': base_f1 + 0.02,
            'precision': base_f1 - 0.01,
            'recall': base_f1,
            'training_time': config['params'] * 0.08,  # Longer due to communication
            'communication_cost': config['params'] * 10 * 0.1,  # MB per round
            'convergence_rounds': np.random.randint(7, 11),
            'per_class': [base_f1 + np.random.uniform(-0.05, 0.05) for _ in range(5)]
        }

    # ViT Models (image-based, better performance)
    for model_name, config in VIT_MODELS.items():
        base_f1 = 0.75 + np.random.uniform(0, 0.10)

        results['vit']['centralized'][model_name] = {
            'f1_macro': base_f1 + 0.12,
            'f1_micro': base_f1 + 0.13,
            'accuracy': base_f1 + 0.14,
            'precision': base_f1 + 0.11,
            'recall': base_f1 + 0.12,
            'training_time': config['params'] * 0.06,
            'communication_cost': 0,
            'convergence_rounds': 1,
            'per_class': [base_f1 + 0.12 + np.random.uniform(-0.05, 0.05) for _ in range(5)]
        }

        results['vit']['federated'][model_name] = {
            'f1_macro': base_f1,
            'f1_micro': base_f1 + 0.01,
            'accuracy': base_f1 + 0.02,
            'precision': base_f1 - 0.01,
            'recall': base_f1,
            'training_time': config['params'] * 0.09,
            'communication_cost': config['params'] * 10 * 0.15,
            'convergence_rounds': np.random.randint(6, 10),
            'per_class': [base_f1 + np.random.uniform(-0.05, 0.05) for _ in range(5)]
        }

    # VLM Models (multimodal, best performance)
    for model_name, config in VLM_MODELS.items():
        base_f1 = 0.78 + np.random.uniform(0, 0.08)

        results['vlm']['centralized'][model_name] = {
            'f1_macro': base_f1 + 0.10,
            'f1_micro': base_f1 + 0.11,
            'accuracy': base_f1 + 0.12,
            'precision': base_f1 + 0.09,
            'recall': base_f1 + 0.10,
            'training_time': config['params'] * 0.04,
            'communication_cost': 0,
            'convergence_rounds': 1,
            'per_class': [base_f1 + 0.10 + np.random.uniform(-0.04, 0.04) for _ in range(5)]
        }

        results['vlm']['federated'][model_name] = {
            'f1_macro': base_f1,
            'f1_micro': base_f1 + 0.01,
            'accuracy': base_f1 + 0.02,
            'precision': base_f1 - 0.01,
            'recall': base_f1,
            'training_time': config['params'] * 0.06,
            'communication_cost': config['params'] * 10 * 0.12,
            'convergence_rounds': np.random.randint(5, 9),
            'per_class': [base_f1 + np.random.uniform(-0.04, 0.04) for _ in range(5)]
        }

    return results


# ============================================================================
# Section 2: Inter-Category Comparison (LLM vs ViT vs VLM)
# ============================================================================

def plot_inter_category_comparison(results, output_dir='plots/comparison'):
    """Plot 1: Overall comparison across model categories."""
    os.makedirs(output_dir, exist_ok=True)

    fig, axes = plt.subplots(2, 2, figsize=(16, 12))
    fig.suptitle('Inter-Category Comparison: LLM vs ViT vs VLM', fontsize=16, fontweight='bold')

    categories = ['LLM', 'ViT', 'VLM']
    colors = [IEEE_COLORS['blue'], IEEE_COLORS['orange'], IEEE_COLORS['green']]

    # Aggregate data
    cat_data = {'LLM': [], 'ViT': [], 'VLM': []}

    for cat_name, cat_key in [('LLM', 'llm'), ('ViT', 'vit'), ('VLM', 'vlm')]:
        for paradigm in ['centralized', 'federated']:
            for model_results in results[cat_key][paradigm].values():
                cat_data[cat_name].append(model_results)

    # Plot 1: F1-Score Distribution
    ax = axes[0, 0]
    f1_data = [[r['f1_macro'] for r in cat_data[cat]] for cat in categories]
    bp = ax.boxplot(f1_data, labels=categories, patch_artist=True, widths=0.6)
    for patch, color in zip(bp['boxes'], colors):
        patch.set_facecolor(color)
        patch.set_alpha(0.7)
    ax.set_ylabel('F1-Score (Macro)', fontweight='bold')
    ax.set_title('(a) F1-Score Distribution by Category', fontweight='bold')
    ax.grid(axis='y', alpha=0.3)

    # Plot 2: Accuracy Distribution
    ax = axes[0, 1]
    acc_data = [[r['accuracy'] for r in cat_data[cat]] for cat in categories]
    bp = ax.boxplot(acc_data, labels=categories, patch_artist=True, widths=0.6)
    for patch, color in zip(bp['boxes'], colors):
        patch.set_facecolor(color)
        patch.set_alpha(0.7)
    ax.set_ylabel('Accuracy', fontweight='bold')
    ax.set_title('(b) Accuracy Distribution by Category', fontweight='bold')
    ax.grid(axis='y', alpha=0.3)

    # Plot 3: Average Metrics Comparison
    ax = axes[1, 0]
    metrics = ['F1', 'Accuracy', 'Precision', 'Recall']
    x = np.arange(len(metrics))
    width = 0.25

    for i, (cat, color) in enumerate(zip(categories, colors)):
        cat_key = cat.lower()
        avg_metrics = []
        for metric_key in ['f1_macro', 'accuracy', 'precision', 'recall']:
            all_vals = []
            for paradigm in ['centralized', 'federated']:
                all_vals.extend([r[metric_key] for r in results[cat_key][paradigm].values()])
            avg_metrics.append(np.mean(all_vals))

        ax.bar(x + i*width, avg_metrics, width, label=cat, color=color, alpha=0.8)

    ax.set_xlabel('Metrics', fontweight='bold')
    ax.set_ylabel('Score', fontweight='bold')
    ax.set_title('(c) Average Metrics by Category', fontweight='bold')
    ax.set_xticks(x + width)
    ax.set_xticklabels(metrics)
    ax.legend()
    ax.grid(axis='y', alpha=0.3)

    # Plot 4: Training Efficiency
    ax = axes[1, 1]
    time_data = [[r['training_time'] for r in cat_data[cat]] for cat in categories]
    positions = np.arange(len(categories))
    bp = ax.boxplot(time_data, positions=positions, patch_artist=True, widths=0.5)
    for patch, color in zip(bp['boxes'], colors):
        patch.set_facecolor(color)
        patch.set_alpha(0.7)

    ax.set_ylabel('Training Time (minutes)', fontweight='bold')
    ax.set_title('(d) Training Efficiency by Category', fontweight='bold')
    ax.set_xticks(positions)
    ax.set_xticklabels(categories)
    ax.grid(axis='y', alpha=0.3)

    plt.tight_layout()
    plt.savefig(f'{output_dir}/01_inter_category_comparison.png', dpi=300, bbox_inches='tight')
    plt.close()
    print(f"‚úì Saved: 01_inter_category_comparison.png")


# ============================================================================
# Section 3: Intra-Category Comparison (Within Each Type)
# ============================================================================

def plot_intra_category_llm(results, output_dir='plots/comparison'):
    """Plot 2: Comparison within LLM models."""

    fig, axes = plt.subplots(2, 2, figsize=(16, 12))
    fig.suptitle('Intra-Category Comparison: LLM Models', fontsize=16, fontweight='bold')

    llm_results = results['llm']
    model_names = list(LLM_MODELS.keys())

    # Plot 1: F1-Score Comparison (Centralized vs Federated)
    ax = axes[0, 0]
    x = np.arange(len(model_names))
    width = 0.35

    cent_f1 = [llm_results['centralized'][m]['f1_macro'] for m in model_names]
    fed_f1 = [llm_results['federated'][m]['f1_macro'] for m in model_names]

    ax.bar(x - width/2, cent_f1, width, label='Centralized', color=IEEE_COLORS['red'], alpha=0.8)
    ax.bar(x + width/2, fed_f1, width, label='Federated', color=IEEE_COLORS['blue'], alpha=0.8)

    ax.set_xlabel('LLM Models', fontweight='bold')
    ax.set_ylabel('F1-Score (Macro)', fontweight='bold')
    ax.set_title('(a) F1-Score: Centralized vs Federated', fontweight='bold')
    ax.set_xticks(x)
    ax.set_xticklabels([m[:15] for m in model_names], rotation=45, ha='right', fontsize=8)
    ax.legend()
    ax.grid(axis='y', alpha=0.3)

    # Plot 2: Model Size vs Performance
    ax = axes[0, 1]
    sizes = [LLM_MODELS[m]['params'] for m in model_names]

    ax.scatter(sizes, cent_f1, s=150, alpha=0.7, color=IEEE_COLORS['red'],
               edgecolors='black', linewidth=1.5, label='Centralized')
    ax.scatter(sizes, fed_f1, s=150, alpha=0.7, color=IEEE_COLORS['blue'],
               edgecolors='black', linewidth=1.5, label='Federated')

    for i, name in enumerate(model_names):
        ax.annotate(name[:8], (sizes[i], cent_f1[i]), fontsize=7, alpha=0.7)

    ax.set_xlabel('Model Size (M parameters)', fontweight='bold')
    ax.set_ylabel('F1-Score (Macro)', fontweight='bold')
    ax.set_title('(b) Size vs Performance Trade-off', fontweight='bold')
    ax.legend()
    ax.grid(True, alpha=0.3)

    # Plot 3: Architecture Type Comparison
    ax = axes[1, 0]
    arch_types = {}
    for m in model_names:
        arch = LLM_MODELS[m]['type']
        if arch not in arch_types:
            arch_types[arch] = {'cent': [], 'fed': []}
        arch_types[arch]['cent'].append(llm_results['centralized'][m]['f1_macro'])
        arch_types[arch]['fed'].append(llm_results['federated'][m]['f1_macro'])

    x = np.arange(len(arch_types))
    width = 0.35
    arch_names = list(arch_types.keys())

    cent_avg = [np.mean(arch_types[a]['cent']) for a in arch_names]
    fed_avg = [np.mean(arch_types[a]['fed']) for a in arch_names]

    ax.bar(x - width/2, cent_avg, width, label='Centralized', color=IEEE_COLORS['red'], alpha=0.8)
    ax.bar(x + width/2, fed_avg, width, label='Federated', color=IEEE_COLORS['blue'], alpha=0.8)

    ax.set_xlabel('Architecture Type', fontweight='bold')
    ax.set_ylabel('Average F1-Score', fontweight='bold')
    ax.set_title('(c) Performance by Architecture Type', fontweight='bold')
    ax.set_xticks(x)
    ax.set_xticklabels(arch_names, rotation=0)
    ax.legend()
    ax.grid(axis='y', alpha=0.3)

    # Plot 4: Convergence Rounds
    ax = axes[1, 1]
    conv_rounds = [llm_results['federated'][m]['convergence_rounds'] for m in model_names]

    ax.bar(range(len(model_names)), conv_rounds, color=IEEE_COLORS['blue'], alpha=0.8)
    ax.set_xlabel('LLM Models', fontweight='bold')
    ax.set_ylabel('Convergence Rounds', fontweight='bold')
    ax.set_title('(d) Federated Convergence Efficiency', fontweight='bold')
    ax.set_xticks(range(len(model_names)))
    ax.set_xticklabels([m[:15] for m in model_names], rotation=45, ha='right', fontsize=8)
    ax.grid(axis='y', alpha=0.3)

    plt.tight_layout()
    plt.savefig(f'{output_dir}/02_intra_category_llm.png', dpi=300, bbox_inches='tight')
    plt.close()
    print(f"‚úì Saved: 02_intra_category_llm.png")


def plot_intra_category_vit(results, output_dir='plots/comparison'):
    """Plot 3: Comparison within ViT models."""

    fig, axes = plt.subplots(2, 2, figsize=(16, 12))
    fig.suptitle('Intra-Category Comparison: ViT Models', fontsize=16, fontweight='bold')

    vit_results = results['vit']
    model_names = list(VIT_MODELS.keys())

    # Plot 1: Performance Comparison
    ax = axes[0, 0]
    x = np.arange(len(model_names))
    width = 0.35

    cent_f1 = [vit_results['centralized'][m]['f1_macro'] for m in model_names]
    fed_f1 = [vit_results['federated'][m]['f1_macro'] for m in model_names]

    ax.bar(x - width/2, cent_f1, width, label='Centralized', color=IEEE_COLORS['red'], alpha=0.8)
    ax.bar(x + width/2, fed_f1, width, label='Federated', color=IEEE_COLORS['orange'], alpha=0.8)

    ax.set_xlabel('ViT Models', fontweight='bold')
    ax.set_ylabel('F1-Score (Macro)', fontweight='bold')
    ax.set_title('(a) F1-Score: Centralized vs Federated', fontweight='bold')
    ax.set_xticks(x)
    ax.set_xticklabels(model_names, rotation=45, ha='right')
    ax.legend()
    ax.grid(axis='y', alpha=0.3)

    # Plot 2: Image Resolution Impact
    ax = axes[0, 1]
    resolutions = [VIT_MODELS[m]['image_size'] for m in model_names]

    ax.scatter(resolutions, cent_f1, s=150, alpha=0.7, color=IEEE_COLORS['red'],
               edgecolors='black', linewidth=1.5, label='Centralized')
    ax.scatter(resolutions, fed_f1, s=150, alpha=0.7, color=IEEE_COLORS['orange'],
               edgecolors='black', linewidth=1.5, label='Federated')

    for i, name in enumerate(model_names):
        ax.annotate(name, (resolutions[i], cent_f1[i]), fontsize=8, alpha=0.7)

    ax.set_xlabel('Input Resolution (pixels)', fontweight='bold')
    ax.set_ylabel('F1-Score (Macro)', fontweight='bold')
    ax.set_title('(b) Resolution Impact on Performance', fontweight='bold')
    ax.legend()
    ax.grid(True, alpha=0.3)

    # Plot 3: Multi-metric Comparison
    ax = axes[1, 0]
    metrics = ['F1', 'Accuracy', 'Precision', 'Recall']
    x = np.arange(len(model_names))
    width = 0.2

    for i, metric_key in enumerate(['f1_macro', 'accuracy', 'precision', 'recall']):
        fed_vals = [vit_results['federated'][m][metric_key] for m in model_names]
        ax.bar(x + i*width, fed_vals, width, label=metrics[i], alpha=0.8)

    ax.set_xlabel('ViT Models', fontweight='bold')
    ax.set_ylabel('Score', fontweight='bold')
    ax.set_title('(c) Multi-Metric Comparison (Federated)', fontweight='bold')
    ax.set_xticks(x + width * 1.5)
    ax.set_xticklabels(model_names, rotation=45, ha='right')
    ax.legend()
    ax.grid(axis='y', alpha=0.3)

    # Plot 4: Communication Cost
    ax = axes[1, 1]
    comm_cost = [vit_results['federated'][m]['communication_cost'] for m in model_names]

    ax.bar(range(len(model_names)), comm_cost, color=IEEE_COLORS['orange'], alpha=0.8)
    ax.set_xlabel('ViT Models', fontweight='bold')
    ax.set_ylabel('Communication Cost (MB/round)', fontweight='bold')
    ax.set_title('(d) Federated Communication Overhead', fontweight='bold')
    ax.set_xticks(range(len(model_names)))
    ax.set_xticklabels(model_names, rotation=45, ha='right')
    ax.grid(axis='y', alpha=0.3)

    plt.tight_layout()
    plt.savefig(f'{output_dir}/03_intra_category_vit.png', dpi=300, bbox_inches='tight')
    plt.close()
    print(f"‚úì Saved: 03_intra_category_vit.png")


def plot_intra_category_vlm(results, output_dir='plots/comparison'):
    """Plot 4: Comparison within VLM models."""

    fig, axes = plt.subplots(2, 2, figsize=(16, 12))
    fig.suptitle('Intra-Category Comparison: VLM Models', fontsize=16, fontweight='bold')

    vlm_results = results['vlm']
    model_names = list(VLM_MODELS.keys())

    # Plot 1: Performance Comparison
    ax = axes[0, 0]
    x = np.arange(len(model_names))
    width = 0.35

    cent_f1 = [vlm_results['centralized'][m]['f1_macro'] for m in model_names]
    fed_f1 = [vlm_results['federated'][m]['f1_macro'] for m in model_names]

    ax.bar(x - width/2, cent_f1, width, label='Centralized', color=IEEE_COLORS['red'], alpha=0.8)
    ax.bar(x + width/2, fed_f1, width, label='Federated', color=IEEE_COLORS['green'], alpha=0.8)

    ax.set_xlabel('VLM Models', fontweight='bold')
    ax.set_ylabel('F1-Score (Macro)', fontweight='bold')
    ax.set_title('(a) F1-Score: Centralized vs Federated', fontweight='bold')
    ax.set_xticks(x)
    ax.set_xticklabels(model_names, rotation=45, ha='right')
    ax.legend()
    ax.grid(axis='y', alpha=0.3)

    # Plot 2: VLM Type Comparison
    ax = axes[0, 1]
    vlm_types = {}
    for m in model_names:
        vlm_type = VLM_MODELS[m]['type']
        if vlm_type not in vlm_types:
            vlm_types[vlm_type] = {'cent': [], 'fed': []}
        vlm_types[vlm_type]['cent'].append(vlm_results['centralized'][m]['f1_macro'])
        vlm_types[vlm_type]['fed'].append(vlm_results['federated'][m]['f1_macro'])

    x = np.arange(len(vlm_types))
    width = 0.35
    type_names = list(vlm_types.keys())

    cent_avg = [np.mean(vlm_types[t]['cent']) for t in type_names]
    fed_avg = [np.mean(vlm_types[t]['fed']) for t in type_names]

    ax.bar(x - width/2, cent_avg, width, label='Centralized', color=IEEE_COLORS['red'], alpha=0.8)
    ax.bar(x + width/2, fed_avg, width, label='Federated', color=IEEE_COLORS['green'], alpha=0.8)

    ax.set_xlabel('VLM Type', fontweight='bold')
    ax.set_ylabel('Average F1-Score', fontweight='bold')
    ax.set_title('(b) Contrastive vs Generative VLMs', fontweight='bold')
    ax.set_xticks(x)
    ax.set_xticklabels([t.capitalize() for t in type_names])
    ax.legend()
    ax.grid(axis='y', alpha=0.3)

    # Plot 3: Model Size Impact
    ax = axes[1, 0]
    sizes = [VLM_MODELS[m]['params'] for m in model_names]

    ax.scatter(sizes, cent_f1, s=200, alpha=0.7, color=IEEE_COLORS['red'],
               edgecolors='black', linewidth=1.5, label='Centralized', marker='o')
    ax.scatter(sizes, fed_f1, s=200, alpha=0.7, color=IEEE_COLORS['green'],
               edgecolors='black', linewidth=1.5, label='Federated', marker='s')

    for i, name in enumerate(model_names):
        ax.annotate(name, (sizes[i], cent_f1[i]), fontsize=8, alpha=0.7,
                   xytext=(5, 5), textcoords='offset points')

    ax.set_xlabel('Model Size (M parameters)', fontweight='bold')
    ax.set_ylabel('F1-Score (Macro)', fontweight='bold')
    ax.set_title('(c) Model Scale vs Performance', fontweight='bold')
    ax.set_xscale('log')
    ax.legend()
    ax.grid(True, alpha=0.3)

    # Plot 4: Training Efficiency
    ax = axes[1, 1]
    train_times_cent = [vlm_results['centralized'][m]['training_time'] for m in model_names]
    train_times_fed = [vlm_results['federated'][m]['training_time'] for m in model_names]

    x = np.arange(len(model_names))
    width = 0.35

    ax.bar(x - width/2, train_times_cent, width, label='Centralized', color=IEEE_COLORS['red'], alpha=0.8)
    ax.bar(x + width/2, train_times_fed, width, label='Federated', color=IEEE_COLORS['green'], alpha=0.8)

    ax.set_xlabel('VLM Models', fontweight='bold')
    ax.set_ylabel('Training Time (minutes)', fontweight='bold')
    ax.set_title('(d) Training Time Comparison', fontweight='bold')
    ax.set_xticks(x)
    ax.set_xticklabels(model_names, rotation=45, ha='right')
    ax.legend()
    ax.grid(axis='y', alpha=0.3)

    plt.tight_layout()
    plt.savefig(f'{output_dir}/04_intra_category_vlm.png', dpi=300, bbox_inches='tight')
    plt.close()
    print(f"‚úì Saved: 04_intra_category_vlm.png")


# ============================================================================
# Section 4: Centralized vs Federated Paradigm Comparison
# ============================================================================

def plot_centralized_vs_federated_detailed(results, output_dir='plots/comparison'):
    """Plot 5: Detailed centralized vs federated comparison for ALL models."""

    fig = plt.figure(figsize=(18, 12))
    gs = GridSpec(3, 3, figure=fig, hspace=0.3, wspace=0.3)
    fig.suptitle('Centralized vs Federated Learning: Comprehensive Comparison',
                 fontsize=16, fontweight='bold')

    # Collect all model data
    all_models = []
    for cat_key in ['llm', 'vit', 'vlm']:
        for model_name in results[cat_key]['centralized'].keys():
            cent_data = results[cat_key]['centralized'][model_name]
            fed_data = results[cat_key]['federated'][model_name]
            all_models.append({
                'name': model_name,
                'category': cat_key.upper(),
                'cent_f1': cent_data['f1_macro'],
                'fed_f1': fed_data['f1_macro'],
                'cent_acc': cent_data['accuracy'],
                'fed_acc': fed_data['accuracy'],
                'privacy_gap': cent_data['f1_macro'] - fed_data['f1_macro'],
                'comm_cost': fed_data['communication_cost'],
                'conv_rounds': fed_data['convergence_rounds']
            })

    # Plot 1: Overall F1 Comparison
    ax1 = fig.add_subplot(gs[0, :2])
    model_names = [m['name'][:15] for m in all_models]
    x = np.arange(len(all_models))
    width = 0.4

    cent_f1s = [m['cent_f1'] for m in all_models]
    fed_f1s = [m['fed_f1'] for m in all_models]

    bars1 = ax1.bar(x - width/2, cent_f1s, width, label='Centralized',
                    color=IEEE_COLORS['red'], alpha=0.8, edgecolor='black')
    bars2 = ax1.bar(x + width/2, fed_f1s, width, label='Federated',
                    color=IEEE_COLORS['blue'], alpha=0.8, edgecolor='black')

    ax1.set_ylabel('F1-Score (Macro)', fontweight='bold')
    ax1.set_title('(a) F1-Score: Centralized vs Federated (All Models)', fontweight='bold')
    ax1.set_xticks(x)
    ax1.set_xticklabels(model_names, rotation=90, ha='center', fontsize=7)
    ax1.legend(loc='lower left')
    ax1.grid(axis='y', alpha=0.3)
    ax1.axhline(y=0.8, color='green', linestyle='--', alpha=0.5, linewidth=1)

    # Plot 2: Privacy-Utility Gap
    ax2 = fig.add_subplot(gs[0, 2])
    privacy_gaps = [m['privacy_gap'] for m in all_models]
    categories = [m['category'] for m in all_models]

    colors_gap = [IEEE_COLORS['blue'] if cat == 'LLM' else
                  IEEE_COLORS['orange'] if cat == 'ViT' else
                  IEEE_COLORS['green'] for cat in categories]

    ax2.barh(range(len(all_models)), privacy_gaps, color=colors_gap, alpha=0.7)
    ax2.set_yticks(range(len(all_models)))
    ax2.set_yticklabels(model_names, fontsize=7)
    ax2.set_xlabel('Privacy-Utility Gap (F1 points)', fontweight='bold', fontsize=9)
    ax2.set_title('(b) Privacy Cost', fontweight='bold')
    ax2.grid(axis='x', alpha=0.3)
    ax2.axvline(x=np.mean(privacy_gaps), color='red', linestyle='--', alpha=0.7)

    # Plot 3: Category-wise Privacy Gap
    ax3 = fig.add_subplot(gs[1, 0])
    cat_gaps = {'LLM': [], 'ViT': [], 'VLM': []}
    for m in all_models:
        cat_gaps[m['category']].append(m['privacy_gap'])

    avg_gaps = [np.mean(cat_gaps[cat]) for cat in ['LLM', 'ViT', 'VLM']]
    std_gaps = [np.std(cat_gaps[cat]) for cat in ['LLM', 'ViT', 'VLM']]

    ax3.bar(['LLM', 'ViT', 'VLM'], avg_gaps, yerr=std_gaps, capsize=10,
            color=[IEEE_COLORS['blue'], IEEE_COLORS['orange'], IEEE_COLORS['green']],
            alpha=0.8, edgecolor='black')
    ax3.set_ylabel('Avg Privacy Gap (F1)', fontweight='bold')
    ax3.set_title('(c) Average Privacy Cost by Category', fontweight='bold')
    ax3.grid(axis='y', alpha=0.3)

    # Plot 4: Scatter - Privacy Gap vs Communication Cost
    ax4 = fig.add_subplot(gs[1, 1])
    comm_costs = [m['comm_cost'] for m in all_models]

    for cat, color in [('LLM', IEEE_COLORS['blue']),
                       ('ViT', IEEE_COLORS['orange']),
                       ('VLM', IEEE_COLORS['green'])]:
        cat_models = [m for m in all_models if m['category'] == cat]
        x_data = [m['comm_cost'] for m in cat_models]
        y_data = [m['privacy_gap'] for m in cat_models]
        ax4.scatter(x_data, y_data, s=150, alpha=0.7, color=color,
                   edgecolors='black', linewidth=1.5, label=cat)

    ax4.set_xlabel('Communication Cost (MB/round)', fontweight='bold')
    ax4.set_ylabel('Privacy Gap (F1)', fontweight='bold')
    ax4.set_title('(d) Communication Cost vs Privacy Trade-off', fontweight='bold')
    ax4.legend()
    ax4.grid(True, alpha=0.3)

    # Plot 5: Convergence Rounds
    ax5 = fig.add_subplot(gs[1, 2])
    conv_rounds = [m['conv_rounds'] for m in all_models]

    ax5.bar(range(len(all_models)), conv_rounds, color=colors_gap, alpha=0.7)
    ax5.set_ylabel('Convergence Rounds', fontweight='bold')
    ax5.set_title('(e) Federated Convergence Speed', fontweight='bold')
    ax5.set_xticks(range(len(all_models)))
    ax5.set_xticklabels(model_names, rotation=90, ha='center', fontsize=7)
    ax5.grid(axis='y', alpha=0.3)
    ax5.axhline(y=np.mean(conv_rounds), color='red', linestyle='--', alpha=0.7)

    # Plot 6: Accuracy Comparison
    ax6 = fig.add_subplot(gs[2, 0])
    cent_accs = [m['cent_acc'] for m in all_models]
    fed_accs = [m['fed_acc'] for m in all_models]

    ax6.scatter(cent_accs, fed_accs, s=150, alpha=0.7,
               c=[IEEE_COLORS['blue'] if m['category'] == 'LLM' else
                  IEEE_COLORS['orange'] if m['category'] == 'ViT' else
                  IEEE_COLORS['green'] for m in all_models],
               edgecolors='black', linewidth=1.5)

    # Diagonal line
    lims = [0.6, 1.0]
    ax6.plot(lims, lims, 'k--', alpha=0.3, linewidth=2)

    ax6.set_xlabel('Centralized Accuracy', fontweight='bold')
    ax6.set_ylabel('Federated Accuracy', fontweight='bold')
    ax6.set_title('(f) Accuracy: Centralized vs Federated', fontweight='bold')
    ax6.grid(True, alpha=0.3)

    # Plot 7: Statistical Summary
    ax7 = fig.add_subplot(gs[2, 1:])
    ax7.axis('off')

    # Calculate statistics
    avg_cent_f1 = np.mean([m['cent_f1'] for m in all_models])
    avg_fed_f1 = np.mean([m['fed_f1'] for m in all_models])
    avg_gap = np.mean(privacy_gaps)
    total_models = len(all_models)

    stats_text = f"""
    STATISTICAL SUMMARY
    ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

    Total Models Compared: {total_models}
    ‚îú‚îÄ LLM Models: {len([m for m in all_models if m['category'] == 'LLM'])}
    ‚îú‚îÄ ViT Models: {len([m for m in all_models if m['category'] == 'ViT'])}
    ‚îî‚îÄ VLM Models: {len([m for m in all_models if m['category'] == 'VLM'])}

    PERFORMANCE COMPARISON
    ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    Average Centralized F1:  {avg_cent_f1:.4f}
    Average Federated F1:    {avg_fed_f1:.4f}
    Privacy-Utility Gap:     {avg_gap:.4f} ({avg_gap/avg_cent_f1*100:.2f}%)

    BEST MODELS
    ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    Best Centralized:  {max(all_models, key=lambda x: x['cent_f1'])['name'][:20]} (F1: {max([m['cent_f1'] for m in all_models]):.4f})
    Best Federated:    {max(all_models, key=lambda x: x['fed_f1'])['name'][:20]} (F1: {max([m['fed_f1'] for m in all_models]):.4f})
    Lowest Privacy Gap: {min(all_models, key=lambda x: x['privacy_gap'])['name'][:20]} (Gap: {min(privacy_gaps):.4f})

    CONVERGENCE
    ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    Avg Convergence Rounds: {np.mean(conv_rounds):.1f}
    Fastest Convergence:    {min(conv_rounds)} rounds
    Slowest Convergence:    {max(conv_rounds)} rounds

    COMMUNICATION
    ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    Avg Communication Cost: {np.mean(comm_costs):.2f} MB/round
    Min Communication:      {min(comm_costs):.2f} MB/round
    Max Communication:      {max(comm_costs):.2f} MB/round
    """

    ax7.text(0.05, 0.95, stats_text, transform=ax7.transAxes,
            fontsize=9, verticalalignment='top', family='monospace',
            bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.3))

    plt.savefig(f'{output_dir}/05_centralized_vs_federated_comprehensive.png',
                dpi=300, bbox_inches='tight')
    plt.close()
    print(f"‚úì Saved: 05_centralized_vs_federated_comprehensive.png")


# ============================================================================
# Section 5: Per-Class Performance Comparison
# ============================================================================

def plot_per_class_comparison(results, output_dir='plots/comparison'):
    """Plot 6: Per-class performance across all models and paradigms."""

    class_names = ['Water\nStress', 'Nutrient\nDef', 'Pest\nRisk', 'Disease\nRisk', 'Heat\nStress']

    fig, axes = plt.subplots(2, 3, figsize=(18, 10))
    fig.suptitle('Per-Class Performance Analysis Across All Models',
                 fontsize=16, fontweight='bold')
    axes = axes.flatten()

    # For each class
    for class_idx in range(5):
        ax = axes[class_idx]

        all_models_data = []
        for cat_key in ['llm', 'vit', 'vlm']:
            for model_name in results[cat_key]['centralized'].keys():
                cent_class = results[cat_key]['centralized'][model_name]['per_class'][class_idx]
                fed_class = results[cat_key]['federated'][model_name]['per_class'][class_idx]
                all_models_data.append({
                    'model': f"{cat_key.upper()}:{model_name[:12]}",
                    'cent': cent_class,
                    'fed': fed_class,
                    'category': cat_key.upper()
                })

        # Sort by federated performance
        all_models_data = sorted(all_models_data, key=lambda x: x['fed'], reverse=True)

        x = np.arange(len(all_models_data))
        width = 0.4

        cent_vals = [m['cent'] for m in all_models_data]
        fed_vals = [m['fed'] for m in all_models_data]

        ax.barh(x - width/2, cent_vals, width, label='Centralized',
                color=IEEE_COLORS['red'], alpha=0.7)
        ax.barh(x + width/2, fed_vals, width, label='Federated',
                color=IEEE_COLORS['blue'], alpha=0.7)

        ax.set_xlabel('F1-Score', fontweight='bold', fontsize=9)
        ax.set_title(f'{class_names[class_idx]}', fontweight='bold')
        ax.set_yticks([])  # Too many models to show labels
        ax.grid(axis='x', alpha=0.3)

        if class_idx == 0:
            ax.legend(fontsize=8)

    # Summary plot
    ax = axes[5]
    ax.axis('off')

    # Calculate per-class averages
    class_avgs = []
    for class_idx in range(5):
        cent_avg = []
        fed_avg = []
        for cat_key in ['llm', 'vit', 'vlm']:
            for model_name in results[cat_key]['centralized'].keys():
                cent_avg.append(results[cat_key]['centralized'][model_name]['per_class'][class_idx])
                fed_avg.append(results[cat_key]['federated'][model_name]['per_class'][class_idx])
        class_avgs.append({
            'class': class_names[class_idx],
            'cent': np.mean(cent_avg),
            'fed': np.mean(fed_avg),
            'gap': np.mean(cent_avg) - np.mean(fed_avg)
        })

    summary_text = "PER-CLASS SUMMARY\n" + "="*40 + "\n\n"
    for ca in class_avgs:
        summary_text += f"{ca['class']}:\n"
        summary_text += f"  Centralized: {ca['cent']:.4f}\n"
        summary_text += f"  Federated:   {ca['fed']:.4f}\n"
        summary_text += f"  Gap:         {ca['gap']:.4f}\n\n"

    ax.text(0.1, 0.9, summary_text, transform=ax.transAxes,
           fontsize=10, verticalalignment='top', family='monospace',
           bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.3))

    plt.tight_layout()
    plt.savefig(f'{output_dir}/06_per_class_comparison.png', dpi=300, bbox_inches='tight')
    plt.close()
    print(f"‚úì Saved: 06_per_class_comparison.png")


# ============================================================================
# Section 6: Statistical Significance Tests
# ============================================================================

def plot_statistical_analysis(results, output_dir='plots/comparison'):
    """Plot 7: Statistical significance tests."""

    fig, axes = plt.subplots(2, 2, figsize=(14, 10))
    fig.suptitle('Statistical Significance Analysis', fontsize=16, fontweight='bold')

    # Collect all F1 scores
    cent_scores = []
    fed_scores = []
    categories = []

    for cat_key in ['llm', 'vit', 'vlm']:
        for model_name in results[cat_key]['centralized'].keys():
            cent_scores.append(results[cat_key]['centralized'][model_name]['f1_macro'])
            fed_scores.append(results[cat_key]['federated'][model_name]['f1_macro'])
            categories.append(cat_key.upper())

    # Plot 1: Paired Comparison
    ax = axes[0, 0]
    for i in range(len(cent_scores)):
        color = IEEE_COLORS['blue'] if categories[i] == 'LLM' else \
                IEEE_COLORS['orange'] if categories[i] == 'ViT' else \
                IEEE_COLORS['green']
        ax.plot([1, 2], [cent_scores[i], fed_scores[i]], 'o-',
                color=color, alpha=0.6, linewidth=1.5)

    ax.set_xlim(0.5, 2.5)
    ax.set_xticks([1, 2])
    ax.set_xticklabels(['Centralized', 'Federated'])
    ax.set_ylabel('F1-Score (Macro)', fontweight='bold')
    ax.set_title('(a) Paired Comparison (Each Line = One Model)', fontweight='bold')
    ax.grid(axis='y', alpha=0.3)

    # Plot 2: Distribution Comparison
    ax = axes[0, 1]
    ax.violinplot([cent_scores, fed_scores], positions=[1, 2],
                  showmeans=True, showmedians=True)
    ax.set_xticks([1, 2])
    ax.set_xticklabels(['Centralized', 'Federated'])
    ax.set_ylabel('F1-Score (Macro)', fontweight='bold')
    ax.set_title('(b) Distribution Comparison', fontweight='bold')
    ax.grid(axis='y', alpha=0.3)

    # Plot 3: Statistical Tests Results
    ax = axes[1, 0]
    ax.axis('off')

    # Perform t-test
    t_stat, p_value = stats.ttest_rel(cent_scores, fed_scores)

    # Effect size (Cohen's d)
    mean_diff = np.mean(np.array(cent_scores) - np.array(fed_scores))
    pooled_std = np.sqrt((np.std(cent_scores)**2 + np.std(fed_scores)**2) / 2)
    cohens_d = mean_diff / pooled_std

    stats_text = f"""
    STATISTICAL TESTS
    ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

    Paired t-test:
      t-statistic: {t_stat:.4f}
      p-value:     {p_value:.6f}
      Significant: {'Yes (p < 0.05)' if p_value < 0.05 else 'No (p >= 0.05)'}

    Effect Size (Cohen's d):
      d = {cohens_d:.4f}
      Interpretation: {'Small' if abs(cohens_d) < 0.5 else 'Medium' if abs(cohens_d) < 0.8 else 'Large'}

    Descriptive Statistics:
      Centralized mean: {np.mean(cent_scores):.4f} ¬± {np.std(cent_scores):.4f}
      Federated mean:   {np.mean(fed_scores):.4f} ¬± {np.std(fed_scores):.4f}
      Mean difference:  {mean_diff:.4f}

    Interpretation:
      {'Centralized learning shows statistically' if p_value < 0.05 else 'No statistically'}
      {'significant higher performance than federated' if p_value < 0.05 else 'significant difference between centralized and federated'}
      {'learning (p < 0.05).' if p_value < 0.05 else 'learning (p >= 0.05).'}
    """

    ax.text(0.05, 0.95, stats_text, transform=ax.transAxes,
           fontsize=9, verticalalignment='top', family='monospace',
           bbox=dict(boxstyle='round', facecolor='lightyellow', alpha=0.5))

    # Plot 4: Category-wise Comparison
    ax = axes[1, 1]
    cat_cent = {'LLM': [], 'ViT': [], 'VLM': []}
    cat_fed = {'LLM': [], 'ViT': [], 'VLM': []}

    for i, cat in enumerate(categories):
        cat_cent[cat].append(cent_scores[i])
        cat_fed[cat].append(fed_scores[i])

    x = np.arange(3)
    width = 0.35

    cent_means = [np.mean(cat_cent['LLM']), np.mean(cat_cent['ViT']), np.mean(cat_cent['VLM'])]
    fed_means = [np.mean(cat_fed['LLM']), np.mean(cat_fed['ViT']), np.mean(cat_fed['VLM'])]

    ax.bar(x - width/2, cent_means, width, label='Centralized',
           color=IEEE_COLORS['red'], alpha=0.8)
    ax.bar(x + width/2, fed_means, width, label='Federated',
           color=IEEE_COLORS['blue'], alpha=0.8)

    ax.set_ylabel('Average F1-Score', fontweight='bold')
    ax.set_title('(c) Category-wise Comparison', fontweight='bold')
    ax.set_xticks(x)
    ax.set_xticklabels(['LLM', 'ViT', 'VLM'])
    ax.legend()
    ax.grid(axis='y', alpha=0.3)

    plt.tight_layout()
    plt.savefig(f'{output_dir}/07_statistical_analysis.png', dpi=300, bbox_inches='tight')
    plt.close()
    print(f"‚úì Saved: 07_statistical_analysis.png")


# ============================================================================
# Section 7: Comprehensive Summary Table
# ============================================================================

def generate_comparison_table(results, output_dir='plots/comparison'):
    """Generate comprehensive comparison table as CSV and visualization."""

    # Collect all data
    table_data = []
    for cat_key in ['llm', 'vit', 'vlm']:
        for model_name in results[cat_key]['centralized'].keys():
            cent = results[cat_key]['centralized'][model_name]
            fed = results[cat_key]['federated'][model_name]

            table_data.append({
                'Category': cat_key.upper(),
                'Model': model_name,
                'Cent_F1': cent['f1_macro'],
                'Fed_F1': fed['f1_macro'],
                'Cent_Acc': cent['accuracy'],
                'Fed_Acc': fed['accuracy'],
                'Cent_Prec': cent['precision'],
                'Fed_Prec': fed['precision'],
                'Cent_Rec': cent['recall'],
                'Fed_Rec': fed['recall'],
                'Privacy_Gap': cent['f1_macro'] - fed['f1_macro'],
                'Comm_Cost': fed['communication_cost'],
                'Conv_Rounds': fed['convergence_rounds'],
                'Cent_Time': cent['training_time'],
                'Fed_Time': fed['training_time']
            })

    # Create DataFrame
    df = pd.DataFrame(table_data)

    # Save CSV
    csv_file = f'{output_dir}/comprehensive_comparison_table.csv'
    df.to_csv(csv_file, index=False, float_format='%.4f')
    print(f"‚úì Saved: comprehensive_comparison_table.csv")

    # Visualize table
    fig, ax = plt.subplots(figsize=(20, len(df) * 0.4 + 2))
    ax.axis('off')

    # Select key columns for visualization
    display_cols = ['Category', 'Model', 'Cent_F1', 'Fed_F1', 'Privacy_Gap',
                    'Cent_Acc', 'Fed_Acc', 'Conv_Rounds', 'Comm_Cost']
    df_display = df[display_cols].copy()
    df_display.columns = ['Cat', 'Model', 'C_F1', 'F_F1', 'Gap', 'C_Acc', 'F_Acc', 'Rounds', 'Comm']

    # Format numbers
    for col in ['C_F1', 'F_F1', 'Gap', 'C_Acc', 'F_Acc']:
        df_display[col] = df_display[col].apply(lambda x: f'{x:.4f}')
    df_display['Comm'] = df_display['Comm'].apply(lambda x: f'{x:.1f}')

    # Create table
    table = ax.table(cellText=df_display.values,
                     colLabels=df_display.columns,
                     cellLoc='center',
                     loc='center',
                     bbox=[0, 0, 1, 1])

    table.auto_set_font_size(False)
    table.set_fontsize(8)
    table.scale(1, 2)

    # Color header
    for i in range(len(df_display.columns)):
        table[(0, i)].set_facecolor(IEEE_COLORS['blue'])
        table[(0, i)].set_text_props(weight='bold', color='white')

    # Color rows by category
    for i in range(1, len(df_display) + 1):
        cat = df_display.iloc[i-1]['Cat']
        color = IEEE_COLORS['blue'] if cat == 'LLM' else \
                IEEE_COLORS['orange'] if cat == 'ViT' else \
                IEEE_COLORS['green']
        for j in range(len(df_display.columns)):
            table[(i, j)].set_facecolor(color)
            table[(i, j)].set_alpha(0.2)

    plt.title('Comprehensive Model Comparison Table', fontsize=14, fontweight='bold', pad=20)
    plt.savefig(f'{output_dir}/08_comparison_table_visual.png', dpi=300, bbox_inches='tight')
    plt.close()
    print(f"‚úì Saved: 08_comparison_table_visual.png")

    return df


# ============================================================================
# Main Execution
# ============================================================================

def main():
    """Generate all comparison plots."""

    print("\n" + "="*70)
    print("COMPREHENSIVE MODEL COMPARISON")
    print("="*70)

    output_dir = 'plots/comparison'
    os.makedirs(output_dir, exist_ok=True)

    # Generate results
    print("\nüìä Generating comprehensive results...")
    results = generate_comprehensive_results()

    # Save results
    with open(f'{output_dir}/comparison_results.json', 'w') as f:
        json.dump(results, f, indent=2)
    print("‚úì Saved: comparison_results.json")

    print("\nüé® Generating comparison plots...")

    # Inter-category comparison
    print("\n1. Inter-category comparison (LLM vs ViT vs VLM)...")
    plot_inter_category_comparison(results, output_dir)

    # Intra-category comparisons
    print("\n2. Intra-category comparison (within LLM)...")
    plot_intra_category_llm(results, output_dir)

    print("\n3. Intra-category comparison (within ViT)...")
    plot_intra_category_vit(results, output_dir)

    print("\n4. Intra-category comparison (within VLM)...")
    plot_intra_category_vlm(results, output_dir)

    # Centralized vs Federated
    print("\n5. Centralized vs Federated comprehensive comparison...")
    plot_centralized_vs_federated_detailed(results, output_dir)

    # Per-class analysis
    print("\n6. Per-class performance comparison...")
    plot_per_class_comparison(results, output_dir)

    # Statistical analysis
    print("\n7. Statistical significance analysis...")
    plot_statistical_analysis(results, output_dir)

    # Comparison table
    print("\n8. Generating comparison table...")
    df = generate_comparison_table(results, output_dir)

    print("\n" + "="*70)
    print("‚úÖ ALL COMPARISON PLOTS GENERATED")
    print("="*70)
    print(f"\nOutput directory: {output_dir}/")
    print(f"Total plots: 8")
    print(f"CSV table: comprehensive_comparison_table.csv")
    print(f"Results JSON: comparison_results.json")

    # Print summary statistics
    print("\n" + "="*70)
    print("SUMMARY STATISTICS")
    print("="*70)

    all_cent_f1 = []
    all_fed_f1 = []
    for cat_key in ['llm', 'vit', 'vlm']:
        for model_name in results[cat_key]['centralized'].keys():
            all_cent_f1.append(results[cat_key]['centralized'][model_name]['f1_macro'])
            all_fed_f1.append(results[cat_key]['federated'][model_name]['f1_macro'])

    print(f"\nTotal Models Compared: {len(all_cent_f1)}")
    print(f"Average Centralized F1: {np.mean(all_cent_f1):.4f}")
    print(f"Average Federated F1: {np.mean(all_fed_f1):.4f}")
    print(f"Average Privacy Gap: {np.mean(np.array(all_cent_f1) - np.array(all_fed_f1)):.4f}")
    print(f"\nBest Centralized: {max(all_cent_f1):.4f}")
    print(f"Best Federated: {max(all_fed_f1):.4f}")

    print("\nüéâ Done!")


if __name__ == "__main__":
    main()
