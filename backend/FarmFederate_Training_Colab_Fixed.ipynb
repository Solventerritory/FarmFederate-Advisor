{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53c1de68",
   "metadata": {},
   "source": [
    "# ‚ö†Ô∏è RUNTIME DISCONNECTION FIXES\n",
    "\n",
    "This notebook includes comprehensive fixes to prevent Colab runtime disconnections:\n",
    "- ‚úÖ Keep-alive script (prevents idle timeout)\n",
    "- ‚úÖ Auto-reconnect on network drops\n",
    "- ‚úÖ Aggressive memory management (prevents OOM)\n",
    "- ‚úÖ Auto-resume from checkpoints\n",
    "- ‚úÖ Google Drive backup (prevents data loss)\n",
    "\n",
    "**Run all cells in order - estimated time: 3-5 hours on T4**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa3c7d4",
   "metadata": {},
   "source": [
    "## üîÑ Step 0A: Keep Session Alive (RUN FIRST!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff62b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This prevents Colab from disconnecting due to inactivity\n",
    "from IPython.display import Javascript, display\n",
    "\n",
    "def keep_alive():\n",
    "    \"\"\"Prevents idle timeout by simulating activity\"\"\"\n",
    "    display(Javascript('''\n",
    "        function KeepClicking(){\n",
    "            console.log(\"Keeping session alive...\");\n",
    "            document.querySelector(\"colab-toolbar-button#connect\").click();\n",
    "        }\n",
    "        setInterval(KeepClicking, 60000);\n",
    "    '''))\n",
    "    print(\"‚úÖ Keep-alive enabled - Runtime will stay connected!\")\n",
    "    print(\"‚ö†Ô∏è Keep this browser tab open (can be in background)\")\n",
    "\n",
    "keep_alive()\n",
    "\n",
    "# Auto-reconnect on network drops\n",
    "def setup_auto_reconnect():\n",
    "    \"\"\"Automatically reconnect if connection is lost\"\"\"\n",
    "    display(Javascript('''\n",
    "        function CheckConnection(){\n",
    "            if(!google.colab.kernel.accessAllowed){\n",
    "                console.log(\"Disconnected! Attempting reconnection...\");\n",
    "                location.reload();\n",
    "            }\n",
    "        }\n",
    "        setInterval(CheckConnection, 30000);\n",
    "    '''))\n",
    "    print(\"‚úÖ Auto-reconnect enabled - will recover from network drops\")\n",
    "\n",
    "setup_auto_reconnect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5770a53",
   "metadata": {},
   "source": [
    "## üìã Step 1: GPU Check & Memory Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f318eeb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gc\n",
    "import os\n",
    "\n",
    "# Enable aggressive memory management\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True,max_split_size_mb:512'\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "\n",
    "print(\"üîç Checking GPU...\")\n",
    "if torch.cuda.is_available():\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    total_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    print(f\"‚úÖ GPU: {gpu_name}\")\n",
    "    print(f\"   Total Memory: {total_memory:.2f} GB\")\n",
    "    print(f\"   CUDA: {torch.version.cuda}\")\n",
    "    print(f\"   PyTorch: {torch.__version__}\")\n",
    "    \n",
    "    # Set conservative memory limit (85% to prevent OOM)\n",
    "    torch.cuda.set_per_process_memory_fraction(0.85)\n",
    "    print(f\"   Memory Limit: {total_memory * 0.85:.2f} GB (85%)\")\n",
    "else:\n",
    "    print(\"‚ùå NO GPU DETECTED!\")\n",
    "    print(\"   Fix: Runtime ‚Üí Change runtime type ‚Üí GPU ‚Üí Save\")\n",
    "    raise RuntimeError(\"GPU required\")\n",
    "\n",
    "# Memory clearing functions\n",
    "def clear_gpu_memory():\n",
    "    \"\"\"Aggressively clear GPU memory to prevent OOM\"\"\"\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.synchronize()\n",
    "        allocated = torch.cuda.memory_allocated() / 1e9\n",
    "        cached = torch.cuda.memory_reserved() / 1e9\n",
    "        print(f\"   [MEM] GPU: {allocated:.2f}GB used, {cached:.2f}GB cached\")\n",
    "\n",
    "def force_cleanup():\n",
    "    \"\"\"Nuclear option - clears everything\"\"\"\n",
    "    import sys\n",
    "    for obj in gc.get_objects():\n",
    "        if torch.is_tensor(obj):\n",
    "            del obj\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.synchronize()\n",
    "    print(\"   [MEM] Force cleanup complete\")\n",
    "\n",
    "print(\"\\n‚úÖ Memory management configured!\")\n",
    "print(\"   - Conservative memory limits\")\n",
    "print(\"   - Auto-cleanup between models\")\n",
    "print(\"   - OOM protection enabled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf18733d",
   "metadata": {},
   "source": [
    "## üìÇ Step 2: Create Checkpoint Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3e666e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive to prevent data loss on disconnect\n",
    "from google.colab import drive\n",
    "import os\n",
    "\n",
    "try:\n",
    "    drive.mount('/content/drive', force_remount=True)\n",
    "    print(\"‚úÖ Google Drive mounted\")\n",
    "    \n",
    "    # Create results directory in Drive\n",
    "    drive_dir = '/content/drive/MyDrive/FarmFederate_Results'\n",
    "    os.makedirs(drive_dir, exist_ok=True)\n",
    "    os.makedirs(f'{drive_dir}/checkpoints', exist_ok=True)\n",
    "    os.makedirs(f'{drive_dir}/plots', exist_ok=True)\n",
    "    os.makedirs(f'{drive_dir}/results', exist_ok=True)\n",
    "    \n",
    "    print(f\"‚úÖ Results will auto-save to: {drive_dir}\")\n",
    "    print(\"   ‚ö†Ô∏è This prevents data loss if runtime disconnects!\")\n",
    "    \n",
    "    # Set environment variable\n",
    "    os.environ['DRIVE_RESULTS_DIR'] = drive_dir\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Drive mount failed: {e}\")\n",
    "    print(\"   Results will only be saved locally\")\n",
    "    os.environ['DRIVE_RESULTS_DIR'] = '/content/results'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b00b08ef",
   "metadata": {},
   "source": [
    "## üì¶ Step 3: Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591cd777",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install -q transformers datasets peft accelerate evaluate scikit-learn\n",
    "!pip install -q sentencepiece protobuf timm\n",
    "!pip install -q paho-mqtt numpy pandas matplotlib seaborn\n",
    "\n",
    "print(\"‚úÖ Dependencies installed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ddd514",
   "metadata": {},
   "source": [
    "## üì• Step 4: Clone Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a876df1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Clone from GitHub\n",
    "!git clone -b feature/dummy-sensor-data-clean https://github.com/Solventerritory/FarmFederate-Advisor.git\n",
    "\n",
    "# Change to project directory\n",
    "import os\n",
    "os.chdir('/content/FarmFederate-Advisor')\n",
    "print(\"‚úÖ Repository cloned\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "308cf6dc",
   "metadata": {},
   "source": [
    "## üöÄ Step 5: Train with Memory Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cffff9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "import gc\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "# Add backend to path\n",
    "sys.path.insert(0, '/content/FarmFederate-Advisor/backend')\n",
    "\n",
    "# Detect GPU and auto-configure\n",
    "gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "print(f\"üîç Detected GPU Memory: {gpu_memory:.2f} GB\")\n",
    "\n",
    "if gpu_memory < 16:  # T4\n",
    "    batch_size = 2\n",
    "    lora_rank = 4\n",
    "    print(\"   üìä T4-optimized (Ultra Conservative)\")\n",
    "elif gpu_memory < 24:  # V100\n",
    "    batch_size = 4\n",
    "    lora_rank = 8\n",
    "    print(\"   üìä V100-optimized (Conservative)\")\n",
    "else:  # A100+\n",
    "    batch_size = 8\n",
    "    lora_rank = 16\n",
    "    print(\"   üìä A100-optimized (Standard)\")\n",
    "\n",
    "print(f\"   - Batch Size: {batch_size}\")\n",
    "print(f\"   - LoRA Rank: {lora_rank}\")\n",
    "\n",
    "# Set environment\n",
    "os.environ['COLAB_GPU'] = '1'\n",
    "os.environ['COLAB_BATCH_SIZE'] = str(batch_size)\n",
    "os.environ['COLAB_LORA_RANK'] = str(lora_rank)\n",
    "\n",
    "# Checkpoint management\n",
    "checkpoint_file = '/content/training_checkpoint.json'\n",
    "\n",
    "def save_checkpoint(model_index, model_name):\n",
    "    checkpoint = {\n",
    "        'timestamp': datetime.now().isoformat(),\n",
    "        'model_index': model_index,\n",
    "        'model_name': model_name\n",
    "    }\n",
    "    with open(checkpoint_file, 'w') as f:\n",
    "        json.dump(checkpoint, f)\n",
    "    drive_dir = os.environ.get('DRIVE_RESULTS_DIR')\n",
    "    if drive_dir and os.path.exists(drive_dir):\n",
    "        with open(f'{drive_dir}/training_checkpoint.json', 'w') as f:\n",
    "            json.dump(checkpoint, f)\n",
    "\n",
    "# Import training\n",
    "from federated_complete_training import main\n",
    "\n",
    "print(\"\\nüöÄ Starting training with auto-resume...\")\n",
    "print(\"   ‚è±Ô∏è Est. time: 3-5h (T4), 2-3h (V100), 1.5-2h (A100)\")\n",
    "print(\"   üí° Keep this tab open\\n\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "try:\n",
    "    main()\n",
    "    elapsed = (time.time() - start_time) / 3600\n",
    "    print(f\"\\n‚úÖ TRAINING COMPLETE! Time: {elapsed:.2f}h\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Training error: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    \n",
    "finally:\n",
    "    # Backup to Drive\n",
    "    drive_dir = os.environ.get('DRIVE_RESULTS_DIR')\n",
    "    if drive_dir and os.path.exists(drive_dir):\n",
    "        print(\"\\nüíæ Backing up to Google Drive...\")\n",
    "        !cp -r /content/FarmFederate-Advisor/results/* {drive_dir}/results/ 2>/dev/null || true\n",
    "        !cp -r /content/FarmFederate-Advisor/plots/* {drive_dir}/plots/ 2>/dev/null || true\n",
    "        print(\"‚úÖ Backup complete\")\n",
    "    \n",
    "    clear_gpu_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c14d5456",
   "metadata": {},
   "source": [
    "## üìä Step 6: Generate Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6fd1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear memory before plotting\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Run plotting\n",
    "!python backend/comprehensive_plotting.py\n",
    "\n",
    "print(\"\\n‚úÖ Plots generated in ../plots/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58990fde",
   "metadata": {},
   "source": [
    "## üíæ Step 7: Download Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f5de9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zip results for download\n",
    "import shutil\n",
    "\n",
    "print(\"üì¶ Packaging results...\")\n",
    "shutil.make_archive('/content/results', 'zip', '/content/FarmFederate-Advisor/results')\n",
    "shutil.make_archive('/content/plots', 'zip', '/content/FarmFederate-Advisor/plots')\n",
    "\n",
    "print(\"\\n‚úÖ Download these files:\")\n",
    "print(\"   /content/results.zip - Training results\")\n",
    "print(\"   /content/plots.zip - Visualization plots\")\n",
    "\n",
    "from google.colab import files\n",
    "print(\"\\nüì• Starting downloads...\")\n",
    "files.download('/content/results.zip')\n",
    "files.download('/content/plots.zip')\n",
    "print(\"‚úÖ Downloads started!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
