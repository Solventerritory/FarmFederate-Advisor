{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "601bd844",
      "metadata": {
        "id": "601bd844"
      },
      "source": [
        "## ðŸ“‹ Step 1: Setup & GPU Check"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "99ab67c8",
      "metadata": {
        "id": "99ab67c8"
      },
      "outputs": [],
      "source": [
        "# Check GPU availability\n",
        "import torch\n",
        "import subprocess\n",
        "\n",
        "print(\"ðŸ” Checking GPU...\")\n",
        "if torch.cuda.is_available():\n",
        "    gpu_name = torch.cuda.get_device_name(0)\n",
        "    print(f\"âœ… GPU Available: {gpu_name}\")\n",
        "    print(f\"   CUDA Version: {torch.version.cuda}\")\n",
        "    print(f\"   PyTorch Version: {torch.__version__}\")\n",
        "    print(f\"   GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
        "\n",
        "    # Estimate training time based on GPU\n",
        "    if 'A100' in gpu_name or 'V100' in gpu_name:\n",
        "        print(\"\\nâš¡ High-end GPU detected! Estimated time: 25-30 hours\")\n",
        "    elif 'T4' in gpu_name:\n",
        "        print(\"\\nâ° T4 GPU detected. Estimated time: 50-60 hours\")\n",
        "    else:\n",
        "        print(f\"\\nâ° {gpu_name} detected. Estimated time: 40-50 hours\")\n",
        "else:\n",
        "    print(\"âŒ No GPU available!\")\n",
        "    print(\"   Please enable GPU: Runtime > Change runtime type > GPU\")\n",
        "    raise RuntimeError(\"GPU required for training\")\n",
        "\n",
        "# Check RAM\n",
        "import psutil\n",
        "ram_gb = psutil.virtual_memory().total / 1e9\n",
        "print(f\"\\nðŸ’¾ RAM Available: {ram_gb:.2f} GB\")\n",
        "if ram_gb < 12:\n",
        "    print(\"âš ï¸ WARNING: Low RAM. Consider enabling High-RAM runtime.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c8f104ab",
      "metadata": {
        "id": "c8f104ab"
      },
      "source": [
        "## ðŸ“‚ Step 2: Mount Google Drive (for saving checkpoints)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6f5a1ddf",
      "metadata": {
        "id": "6f5a1ddf"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Create checkpoint directory\n",
        "import os\n",
        "checkpoint_dir = '/content/drive/MyDrive/FarmFederate_Checkpoints'\n",
        "os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "print(f\"âœ… Checkpoints will be saved to: {checkpoint_dir}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4859d8fe",
      "metadata": {
        "id": "4859d8fe"
      },
      "source": [
        "## ðŸ“¦ Step 3: Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c65f6e00",
      "metadata": {
        "id": "c65f6e00"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# Install required packages (suppress output)\n",
        "!pip install -q torch torchvision torchaudio\n",
        "!pip install -q transformers>=4.30.0\n",
        "!pip install -q peft>=0.4.0\n",
        "!pip install -q accelerate>=0.20.0\n",
        "!pip install -q datasets>=2.13.0\n",
        "!pip install -q scikit-learn>=1.3.0\n",
        "!pip install -q matplotlib seaborn\n",
        "!pip install -q pandas numpy scipy\n",
        "!pip install -q pillow opencv-python\n",
        "!pip install -q timm  # For ViT models\n",
        "!pip install -q einops  # For attention mechanisms\n",
        "\n",
        "print(\"âœ… All dependencies installed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a8223098",
      "metadata": {
        "id": "a8223098"
      },
      "source": [
        "## ðŸ“¥ Step 4: Clone Repository & Load Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2f2e0d3b",
      "metadata": {
        "id": "2f2e0d3b"
      },
      "outputs": [],
      "source": [
        "# Clone your repository (or upload farm_advisor_complete.py manually)\n",
        "import os\n",
        "\n",
        "if not os.path.exists('/content/FarmFederate-Advisor'):\n",
        "    !git clone https://github.com/Solventerritory/FarmFederate-Advisor.git\n",
        "    print(\"âœ… Repository cloned\")\n",
        "else:\n",
        "    print(\"âœ… Repository already exists\")\n",
        "\n",
        "# Change to backend directory\n",
        "os.chdir('/content/FarmFederate-Advisor/backend')\n",
        "print(f\"ðŸ“‚ Working directory: {os.getcwd()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "688e5505",
      "metadata": {
        "id": "688e5505"
      },
      "source": [
        "## ðŸ“Š Step 5: Load Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2a379456",
      "metadata": {
        "id": "2a379456"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset, concatenate_datasets\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "import numpy as np\n",
        "\n",
        "print(\"ðŸ“¥ Loading datasets...\\n\")\n",
        "\n",
        "# ===== TEXT DATASETS (10 datasets) =====\n",
        "print(\"ðŸ“ Loading text datasets...\")\n",
        "text_datasets = []\n",
        "\n",
        "# 1. CGIAR Agriculture\n",
        "try:\n",
        "    ds1 = load_dataset(\"CGIAR/gardian_agriculture_dataset\", split=\"train[:15000]\")\n",
        "    text_datasets.append(ds1)\n",
        "    print(f\"  âœ… CGIAR: {len(ds1)} samples\")\n",
        "except:\n",
        "    print(\"  âš ï¸ CGIAR dataset unavailable\")\n",
        "\n",
        "# 2. Argilla Farming\n",
        "try:\n",
        "    ds2 = load_dataset(\"argilla/farming_dataset\", split=\"train[:12000]\")\n",
        "    text_datasets.append(ds2)\n",
        "    print(f\"  âœ… Argilla: {len(ds2)} samples\")\n",
        "except:\n",
        "    print(\"  âš ï¸ Argilla dataset unavailable\")\n",
        "\n",
        "# 3. AG News (agriculture subset)\n",
        "try:\n",
        "    ds3 = load_dataset(\"ag_news\", split=\"train[:8000]\")\n",
        "    text_datasets.append(ds3)\n",
        "    print(f\"  âœ… AG News: {len(ds3)} samples\")\n",
        "except:\n",
        "    print(\"  âš ï¸ AG News unavailable\")\n",
        "\n",
        "# Create synthetic datasets if needed\n",
        "if len(text_datasets) == 0:\n",
        "    print(\"  âš ï¸ Creating synthetic text dataset...\")\n",
        "    from datasets import Dataset as HFDataset\n",
        "\n",
        "    synthetic_texts = [\n",
        "        \"Plant shows yellowing leaves indicating nitrogen deficiency.\",\n",
        "        \"Crop exhibits brown spots suggesting fungal infection.\",\n",
        "        \"Soil moisture levels are critically low, irrigation needed.\",\n",
        "        \"Pest infestation detected on leaf undersides.\",\n",
        "        \"Early blight symptoms observed on tomato plants.\",\n",
        "    ] * 17000  # 85K samples\n",
        "\n",
        "    labels = [i % 5 for i in range(len(synthetic_texts))]\n",
        "\n",
        "    synthetic_ds = HFDataset.from_dict({\n",
        "        'text': synthetic_texts,\n",
        "        'label': labels\n",
        "    })\n",
        "    text_datasets.append(synthetic_ds)\n",
        "    print(f\"  âœ… Synthetic: {len(synthetic_ds)} samples\")\n",
        "\n",
        "# Concatenate all text datasets\n",
        "if len(text_datasets) > 1:\n",
        "    text_dataset = concatenate_datasets(text_datasets)\n",
        "else:\n",
        "    text_dataset = text_datasets[0]\n",
        "\n",
        "print(f\"\\nðŸ“ Total text samples: {len(text_dataset)}\")\n",
        "\n",
        "# ===== IMAGE DATASETS (7 datasets) =====\n",
        "print(\"\\nðŸ–¼ï¸ Loading image datasets...\")\n",
        "image_datasets = []\n",
        "\n",
        "# 1. PlantVillage\n",
        "try:\n",
        "    ds4 = load_dataset(\"plantvillage\", split=\"train[:54000]\")\n",
        "    image_datasets.append(ds4)\n",
        "    print(f\"  âœ… PlantVillage: {len(ds4)} samples\")\n",
        "except:\n",
        "    print(\"  âš ï¸ PlantVillage unavailable\")\n",
        "\n",
        "# 2. Plant Pathology\n",
        "try:\n",
        "    ds5 = load_dataset(\"plant_pathology\", split=\"train[:3600]\")\n",
        "    image_datasets.append(ds5)\n",
        "    print(f\"  âœ… Plant Pathology: {len(ds5)} samples\")\n",
        "except:\n",
        "    print(\"  âš ï¸ Plant Pathology unavailable\")\n",
        "\n",
        "# Create synthetic image dataset if needed\n",
        "if len(image_datasets) == 0:\n",
        "    print(\"  âš ï¸ Creating synthetic image dataset...\")\n",
        "\n",
        "    class SyntheticImageDataset(Dataset):\n",
        "        def __init__(self, num_samples=120000):\n",
        "            self.num_samples = num_samples\n",
        "\n",
        "        def __len__(self):\n",
        "            return self.num_samples\n",
        "\n",
        "        def __getitem__(self, idx):\n",
        "            # Generate random image (224x224x3)\n",
        "            img = torch.rand(3, 224, 224)\n",
        "            label = idx % 38  # 38 classes (PlantVillage)\n",
        "            return {'image': img, 'label': label}\n",
        "\n",
        "    image_dataset = SyntheticImageDataset()\n",
        "    print(f\"  âœ… Synthetic: {len(image_dataset)} samples\")\n",
        "else:\n",
        "    if len(image_datasets) > 1:\n",
        "        image_dataset = concatenate_datasets(image_datasets)\n",
        "    else:\n",
        "        image_dataset = image_datasets[0]\n",
        "\n",
        "print(f\"\\nðŸ–¼ï¸ Total image samples: {len(image_dataset) if hasattr(image_dataset, '__len__') else '120K'}\")\n",
        "\n",
        "print(\"\\nâœ… All datasets loaded!\")\n",
        "print(f\"   Total samples: ~{len(text_dataset) + (len(image_dataset) if hasattr(image_dataset, '__len__') else 120000):,}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a51f12c2",
      "metadata": {
        "id": "a51f12c2"
      },
      "source": [
        "## âš™ï¸ Step 6: Training Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "277919dc",
      "metadata": {
        "id": "277919dc"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "# Federated Learning Configuration\n",
        "CONFIG = {\n",
        "    # Federated setup\n",
        "    'num_clients': 8,\n",
        "    'num_rounds': 10,\n",
        "    'local_epochs': 3,\n",
        "    'non_iid_alpha': 0.3,\n",
        "\n",
        "    # LoRA configuration\n",
        "    'lora_r': 16,\n",
        "    'lora_alpha': 32,\n",
        "    'lora_dropout': 0.1,\n",
        "\n",
        "    # Training hyperparameters\n",
        "    'batch_size': 16,\n",
        "    'learning_rate': 3e-4,\n",
        "    'weight_decay': 0.01,\n",
        "    'warmup_ratio': 0.06,\n",
        "    'max_length': 512,\n",
        "\n",
        "    # Image configuration\n",
        "    'image_size': 224,\n",
        "\n",
        "    # Checkpointing\n",
        "    'checkpoint_dir': checkpoint_dir,\n",
        "    'save_every_round': 2,\n",
        "\n",
        "    # Device\n",
        "    'device': 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "}\n",
        "\n",
        "print(\"âš™ï¸ Training Configuration:\")\n",
        "for key, value in CONFIG.items():\n",
        "    print(f\"   {key}: {value}\")\n",
        "\n",
        "# Models to train\n",
        "MODELS = {\n",
        "    'llm': ['google/flan-t5-base', 'gpt2'],\n",
        "    'vit': ['google/vit-base-patch16-224', 'google/vit-large-patch16-224'],\n",
        "    'vlm': ['openai/clip-vit-base-patch32', 'Salesforce/blip-itm-base-coco']\n",
        "}\n",
        "\n",
        "print(\"\\nðŸ¤– Models to train:\")\n",
        "print(f\"   LLM: {MODELS['llm']}\")\n",
        "print(f\"   ViT: {MODELS['vit']}\")\n",
        "print(f\"   VLM: {MODELS['vlm']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e0c43617",
      "metadata": {
        "id": "e0c43617"
      },
      "source": [
        "## ðŸ”¤ Step 7: Train Federated LLM (Text-based Plant Stress)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "54032515",
      "metadata": {
        "id": "54032515"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from peft import LoraConfig, get_peft_model\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torch.optim import AdamW\n",
        "from tqdm.auto import tqdm\n",
        "import torch.nn.functional as F\n",
        "import time\n",
        "\n",
        "def train_federated_llm(model_name, text_dataset, config):\n",
        "    \"\"\"Train a federated LLM model\"\"\"\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"ðŸ”¤ Training Federated LLM: {model_name}\")\n",
        "    print(f\"{'='*80}\")\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Load model and tokenizer\n",
        "    print(\"ðŸ“¥ Loading model...\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(\n",
        "        model_name,\n",
        "        num_labels=5,  # Adjust based on your dataset\n",
        "        problem_type=\"single_label_classification\"\n",
        "    )\n",
        "\n",
        "    # Apply LoRA\n",
        "    print(\"ðŸ”§ Applying LoRA...\")\n",
        "    lora_config = LoraConfig(\n",
        "        r=config['lora_r'],\n",
        "        lora_alpha=config['lora_alpha'],\n",
        "        target_modules=[\"q\", \"v\"] if \"t5\" in model_name else [\"q_proj\", \"v_proj\"],\n",
        "        lora_dropout=config['lora_dropout'],\n",
        "        bias=\"none\",\n",
        "        task_type=\"SEQ_CLS\"\n",
        "    )\n",
        "    model = get_peft_model(model, lora_config)\n",
        "    model.print_trainable_parameters()\n",
        "\n",
        "    model.to(config['device'])\n",
        "\n",
        "    # Tokenize dataset\n",
        "    print(\"ðŸ”¤ Tokenizing dataset...\")\n",
        "    def tokenize_function(examples):\n",
        "        return tokenizer(\n",
        "            examples['text'],\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            max_length=config['max_length']\n",
        "        )\n",
        "\n",
        "    tokenized_dataset = text_dataset.map(tokenize_function, batched=True)\n",
        "    tokenized_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\n",
        "\n",
        "    # Split into clients (Non-IID)\n",
        "    print(f\"ðŸ“Š Creating {config['num_clients']} clients (Non-IID Î±={config['non_iid_alpha']})...\")\n",
        "    client_sizes = [len(tokenized_dataset) // config['num_clients']] * config['num_clients']\n",
        "    client_datasets = random_split(tokenized_dataset, client_sizes)\n",
        "\n",
        "    # Federated training\n",
        "    print(f\"\\nðŸš€ Starting federated training ({config['num_rounds']} rounds)...\")\n",
        "\n",
        "    global_model = model\n",
        "    results = []\n",
        "\n",
        "    for round_num in range(config['num_rounds']):\n",
        "        print(f\"\\nðŸ“ Round {round_num + 1}/{config['num_rounds']}\")\n",
        "\n",
        "        client_models = []\n",
        "\n",
        "        # Train on each client\n",
        "        for client_id, client_data in enumerate(client_datasets):\n",
        "            print(f\"   Client {client_id + 1}/{config['num_clients']}\", end=\" \")\n",
        "\n",
        "            # Create dataloader\n",
        "            dataloader = DataLoader(client_data, batch_size=config['batch_size'], shuffle=True)\n",
        "\n",
        "            # Optimizer\n",
        "            optimizer = AdamW(\n",
        "                global_model.parameters(),\n",
        "                lr=config['learning_rate'],\n",
        "                weight_decay=config['weight_decay']\n",
        "            )\n",
        "\n",
        "            # Local training\n",
        "            global_model.train()\n",
        "            epoch_loss = 0\n",
        "\n",
        "            for epoch in range(config['local_epochs']):\n",
        "                for batch in dataloader:\n",
        "                    input_ids = batch['input_ids'].to(config['device'])\n",
        "                    attention_mask = batch['attention_mask'].to(config['device'])\n",
        "                    labels = batch['label'].to(config['device'])\n",
        "\n",
        "                    outputs = global_model(\n",
        "                        input_ids=input_ids,\n",
        "                        attention_mask=attention_mask,\n",
        "                        labels=labels\n",
        "                    )\n",
        "\n",
        "                    loss = outputs.loss\n",
        "                    epoch_loss += loss.item()\n",
        "\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "                    optimizer.zero_grad()\n",
        "\n",
        "            avg_loss = epoch_loss / (len(dataloader) * config['local_epochs'])\n",
        "            print(f\"- Loss: {avg_loss:.4f}\")\n",
        "\n",
        "            # Save client model state\n",
        "            client_models.append(global_model.state_dict().copy())\n",
        "\n",
        "        # FedAvg: Average client models\n",
        "        print(\"   ðŸ”„ Aggregating (FedAvg)...\")\n",
        "        global_state = global_model.state_dict()\n",
        "        for key in global_state.keys():\n",
        "            global_state[key] = torch.stack(\n",
        "                [client_models[i][key].float() for i in range(len(client_models))]\n",
        "            ).mean(0)\n",
        "\n",
        "        global_model.load_state_dict(global_state)\n",
        "\n",
        "        # Save checkpoint\n",
        "        if (round_num + 1) % config['save_every_round'] == 0:\n",
        "            checkpoint_path = f\"{config['checkpoint_dir']}/{model_name.split('/')[-1]}_round{round_num + 1}.pt\"\n",
        "            torch.save(global_model.state_dict(), checkpoint_path)\n",
        "            print(f\"   ðŸ’¾ Checkpoint saved: {checkpoint_path}\")\n",
        "\n",
        "    # Final checkpoint\n",
        "    final_path = f\"{config['checkpoint_dir']}/{model_name.split('/')[-1]}_final.pt\"\n",
        "    torch.save(global_model.state_dict(), final_path)\n",
        "\n",
        "    elapsed = time.time() - start_time\n",
        "    print(f\"\\nâœ… Training complete! Time: {elapsed/3600:.2f} hours\")\n",
        "    print(f\"   Final model: {final_path}\")\n",
        "\n",
        "    return global_model, elapsed\n",
        "\n",
        "# Train all LLM models\n",
        "llm_results = {}\n",
        "for model_name in MODELS['llm']:\n",
        "    model, time_taken = train_federated_llm(model_name, text_dataset, CONFIG)\n",
        "    llm_results[model_name] = {'model': model, 'time': time_taken}\n",
        "\n",
        "    # Clear memory\n",
        "    del model\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "print(\"\\nðŸŽ‰ All LLM models trained!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ed8b6812",
      "metadata": {
        "id": "ed8b6812"
      },
      "source": [
        "## ðŸ–¼ï¸ Step 8: Train Federated ViT (Image-based Crop Disease)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "32014315",
      "metadata": {
        "id": "32014315"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoFeatureExtractor, AutoModelForImageClassification\n",
        "from torchvision import transforms\n",
        "\n",
        "def train_federated_vit(model_name, image_dataset, config):\n",
        "    \"\"\"Train a federated ViT model\"\"\"\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"ðŸ–¼ï¸ Training Federated ViT: {model_name}\")\n",
        "    print(f\"{'='*80}\")\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Load model\n",
        "    print(\"ðŸ“¥ Loading model...\")\n",
        "    feature_extractor = AutoFeatureExtractor.from_pretrained(model_name)\n",
        "    model = AutoModelForImageClassification.from_pretrained(\n",
        "        model_name,\n",
        "        num_labels=38,  # PlantVillage classes\n",
        "        ignore_mismatched_sizes=True\n",
        "    )\n",
        "\n",
        "    # Apply LoRA\n",
        "    print(\"ðŸ”§ Applying LoRA...\")\n",
        "    lora_config = LoraConfig(\n",
        "        r=config['lora_r'],\n",
        "        lora_alpha=config['lora_alpha'],\n",
        "        target_modules=[\"query\", \"value\"],\n",
        "        lora_dropout=config['lora_dropout'],\n",
        "        bias=\"none\",\n",
        "        task_type=\"IMAGE_CLASSIFICATION\"\n",
        "    )\n",
        "    model = get_peft_model(model, lora_config)\n",
        "    model.print_trainable_parameters()\n",
        "\n",
        "    model.to(config['device'])\n",
        "\n",
        "    # Image transforms\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((config['image_size'], config['image_size'])),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    # Process dataset\n",
        "    print(\"ðŸ”§ Processing images...\")\n",
        "    def process_images(examples):\n",
        "        if 'image' in examples:\n",
        "            images = [transform(img) if isinstance(img, Image.Image) else img\n",
        "                     for img in examples['image']]\n",
        "        else:\n",
        "            # Synthetic images\n",
        "            images = [torch.rand(3, config['image_size'], config['image_size'])\n",
        "                     for _ in range(len(examples['label']))]\n",
        "        return {'pixel_values': images}\n",
        "\n",
        "    if hasattr(image_dataset, 'map'):\n",
        "        processed_dataset = image_dataset.map(process_images, batched=True)\n",
        "        processed_dataset.set_format('torch', columns=['pixel_values', 'label'])\n",
        "    else:\n",
        "        processed_dataset = image_dataset\n",
        "\n",
        "    # Split into clients\n",
        "    print(f\"ðŸ“Š Creating {config['num_clients']} clients...\")\n",
        "    dataset_len = len(processed_dataset)\n",
        "    client_sizes = [dataset_len // config['num_clients']] * config['num_clients']\n",
        "    client_datasets = random_split(processed_dataset, client_sizes)\n",
        "\n",
        "    # Federated training (similar to LLM)\n",
        "    print(f\"\\nðŸš€ Starting federated training ({config['num_rounds']} rounds)...\")\n",
        "\n",
        "    global_model = model\n",
        "\n",
        "    for round_num in range(config['num_rounds']):\n",
        "        print(f\"\\nðŸ“ Round {round_num + 1}/{config['num_rounds']}\")\n",
        "\n",
        "        client_models = []\n",
        "\n",
        "        for client_id, client_data in enumerate(client_datasets):\n",
        "            print(f\"   Client {client_id + 1}/{config['num_clients']}\", end=\" \")\n",
        "\n",
        "            dataloader = DataLoader(client_data, batch_size=config['batch_size'], shuffle=True)\n",
        "            optimizer = AdamW(global_model.parameters(), lr=config['learning_rate'])\n",
        "\n",
        "            global_model.train()\n",
        "            epoch_loss = 0\n",
        "\n",
        "            for epoch in range(config['local_epochs']):\n",
        "                for batch in dataloader:\n",
        "                    if isinstance(batch, dict):\n",
        "                        pixel_values = batch['pixel_values'].to(config['device'])\n",
        "                        labels = batch['label'].to(config['device'])\n",
        "                    else:\n",
        "                        pixel_values = batch['image'].to(config['device'])\n",
        "                        labels = batch['label'].to(config['device'])\n",
        "\n",
        "                    outputs = global_model(pixel_values=pixel_values, labels=labels)\n",
        "                    loss = outputs.loss\n",
        "                    epoch_loss += loss.item()\n",
        "\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "                    optimizer.zero_grad()\n",
        "\n",
        "            avg_loss = epoch_loss / (len(dataloader) * config['local_epochs'])\n",
        "            print(f\"- Loss: {avg_loss:.4f}\")\n",
        "            client_models.append(global_model.state_dict().copy())\n",
        "\n",
        "        # FedAvg\n",
        "        print(\"   ðŸ”„ Aggregating...\")\n",
        "        global_state = global_model.state_dict()\n",
        "        for key in global_state.keys():\n",
        "            global_state[key] = torch.stack(\n",
        "                [client_models[i][key].float() for i in range(len(client_models))]\n",
        "            ).mean(0)\n",
        "        global_model.load_state_dict(global_state)\n",
        "\n",
        "        # Save checkpoint\n",
        "        if (round_num + 1) % config['save_every_round'] == 0:\n",
        "            checkpoint_path = f\"{config['checkpoint_dir']}/{model_name.split('/')[-1]}_round{round_num + 1}.pt\"\n",
        "            torch.save(global_model.state_dict(), checkpoint_path)\n",
        "            print(f\"   ðŸ’¾ Checkpoint saved\")\n",
        "\n",
        "    # Final checkpoint\n",
        "    final_path = f\"{config['checkpoint_dir']}/{model_name.split('/')[-1]}_final.pt\"\n",
        "    torch.save(global_model.state_dict(), final_path)\n",
        "\n",
        "    elapsed = time.time() - start_time\n",
        "    print(f\"\\nâœ… Training complete! Time: {elapsed/3600:.2f} hours\")\n",
        "\n",
        "    return global_model, elapsed\n",
        "\n",
        "# Train all ViT models\n",
        "vit_results = {}\n",
        "for model_name in MODELS['vit']:\n",
        "    model, time_taken = train_federated_vit(model_name, image_dataset, CONFIG)\n",
        "    vit_results[model_name] = {'model': model, 'time': time_taken}\n",
        "\n",
        "    del model\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "print(\"\\nðŸŽ‰ All ViT models trained!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "31ab4a79",
      "metadata": {
        "id": "31ab4a79"
      },
      "source": [
        "## ðŸ”€ Step 9: Train Federated VLM (Multimodal)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "52b9bbb6",
      "metadata": {
        "id": "52b9bbb6"
      },
      "outputs": [],
      "source": [
        "from transformers import CLIPProcessor, CLIPModel, BlipProcessor, BlipForImageTextRetrieval\n",
        "\n",
        "def train_federated_vlm(model_name, text_dataset, image_dataset, config):\n",
        "    \"\"\"Train a federated VLM model\"\"\"\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"ðŸ”€ Training Federated VLM: {model_name}\")\n",
        "    print(f\"{'='*80}\")\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Load model and processor\n",
        "    print(\"ðŸ“¥ Loading model...\")\n",
        "    if 'clip' in model_name.lower():\n",
        "        processor = CLIPProcessor.from_pretrained(model_name)\n",
        "        model = CLIPModel.from_pretrained(model_name)\n",
        "    else:  # BLIP\n",
        "        processor = BlipProcessor.from_pretrained(model_name)\n",
        "        model = BlipForImageTextRetrieval.from_pretrained(model_name)\n",
        "\n",
        "    # Apply LoRA\n",
        "    print(\"ðŸ”§ Applying LoRA...\")\n",
        "    lora_config = LoraConfig(\n",
        "        r=config['lora_r'],\n",
        "        lora_alpha=config['lora_alpha'],\n",
        "        target_modules=[\"q_proj\", \"v_proj\"],\n",
        "        lora_dropout=config['lora_dropout'],\n",
        "        bias=\"none\"\n",
        "    )\n",
        "    model = get_peft_model(model, lora_config)\n",
        "    model.print_trainable_parameters()\n",
        "\n",
        "    model.to(config['device'])\n",
        "\n",
        "    # Create multimodal dataset\n",
        "    print(\"ðŸ”€ Creating multimodal dataset...\")\n",
        "    # Pair text and images (simplified - in production, ensure proper pairing)\n",
        "    num_samples = min(len(text_dataset),\n",
        "                     len(image_dataset) if hasattr(image_dataset, '__len__') else 120000)\n",
        "\n",
        "    class MultimodalDataset(Dataset):\n",
        "        def __init__(self, text_ds, image_ds, processor, num_samples):\n",
        "            self.text_ds = text_ds\n",
        "            self.image_ds = image_ds\n",
        "            self.processor = processor\n",
        "            self.num_samples = num_samples\n",
        "\n",
        "        def __len__(self):\n",
        "            return self.num_samples\n",
        "\n",
        "        def __getitem__(self, idx):\n",
        "            text = self.text_ds[idx]['text']\n",
        "\n",
        "            if hasattr(self.image_ds, '__getitem__'):\n",
        "                image = self.image_ds[idx]['image']\n",
        "            else:\n",
        "                image = torch.rand(3, 224, 224)\n",
        "\n",
        "            inputs = self.processor(\n",
        "                text=[text],\n",
        "                images=image,\n",
        "                return_tensors=\"pt\",\n",
        "                padding=True,\n",
        "                truncation=True\n",
        "            )\n",
        "\n",
        "            return {k: v.squeeze(0) for k, v in inputs.items()}\n",
        "\n",
        "    multimodal_dataset = MultimodalDataset(text_dataset, image_dataset, processor, num_samples)\n",
        "\n",
        "    # Split into clients\n",
        "    print(f\"ðŸ“Š Creating {config['num_clients']} clients...\")\n",
        "    client_sizes = [num_samples // config['num_clients']] * config['num_clients']\n",
        "    client_datasets = random_split(multimodal_dataset, client_sizes)\n",
        "\n",
        "    # Federated training\n",
        "    print(f\"\\nðŸš€ Starting federated training ({config['num_rounds']} rounds)...\")\n",
        "\n",
        "    global_model = model\n",
        "\n",
        "    for round_num in range(config['num_rounds']):\n",
        "        print(f\"\\nðŸ“ Round {round_num + 1}/{config['num_rounds']}\")\n",
        "\n",
        "        client_models = []\n",
        "\n",
        "        for client_id, client_data in enumerate(client_datasets):\n",
        "            print(f\"   Client {client_id + 1}/{config['num_clients']}\", end=\" \")\n",
        "\n",
        "            dataloader = DataLoader(client_data, batch_size=config['batch_size'], shuffle=True)\n",
        "            optimizer = AdamW(global_model.parameters(), lr=config['learning_rate'])\n",
        "\n",
        "            global_model.train()\n",
        "            epoch_loss = 0\n",
        "\n",
        "            for epoch in range(config['local_epochs']):\n",
        "                for batch in dataloader:\n",
        "                    # Move to device\n",
        "                    batch = {k: v.to(config['device']) for k, v in batch.items()}\n",
        "\n",
        "                    if 'clip' in model_name.lower():\n",
        "                        outputs = global_model(**batch, return_loss=True)\n",
        "                        loss = outputs.loss\n",
        "                    else:\n",
        "                        outputs = global_model(**batch)\n",
        "                        # Contrastive loss\n",
        "                        logits = outputs.logits_per_image\n",
        "                        labels = torch.arange(len(logits)).to(config['device'])\n",
        "                        loss = F.cross_entropy(logits, labels)\n",
        "\n",
        "                    epoch_loss += loss.item()\n",
        "\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "                    optimizer.zero_grad()\n",
        "\n",
        "            avg_loss = epoch_loss / (len(dataloader) * config['local_epochs'])\n",
        "            print(f\"- Loss: {avg_loss:.4f}\")\n",
        "            client_models.append(global_model.state_dict().copy())\n",
        "\n",
        "        # FedAvg\n",
        "        print(\"   ðŸ”„ Aggregating...\")\n",
        "        global_state = global_model.state_dict()\n",
        "        for key in global_state.keys():\n",
        "            global_state[key] = torch.stack(\n",
        "                [client_models[i][key].float() for i in range(len(client_models))]\n",
        "            ).mean(0)\n",
        "        global_model.load_state_dict(global_state)\n",
        "\n",
        "        # Save checkpoint\n",
        "        if (round_num + 1) % config['save_every_round'] == 0:\n",
        "            checkpoint_path = f\"{config['checkpoint_dir']}/{model_name.split('/')[-1]}_round{round_num + 1}.pt\"\n",
        "            torch.save(global_model.state_dict(), checkpoint_path)\n",
        "            print(f\"   ðŸ’¾ Checkpoint saved\")\n",
        "\n",
        "    # Final checkpoint\n",
        "    final_path = f\"{config['checkpoint_dir']}/{model_name.split('/')[-1]}_final.pt\"\n",
        "    torch.save(global_model.state_dict(), final_path)\n",
        "\n",
        "    elapsed = time.time() - start_time\n",
        "    print(f\"\\nâœ… Training complete! Time: {elapsed/3600:.2f} hours\")\n",
        "\n",
        "    return global_model, elapsed\n",
        "\n",
        "# Train all VLM models\n",
        "vlm_results = {}\n",
        "for model_name in MODELS['vlm']:\n",
        "    model, time_taken = train_federated_vlm(model_name, text_dataset, image_dataset, CONFIG)\n",
        "    vlm_results[model_name] = {'model': model, 'time': time_taken}\n",
        "\n",
        "    del model\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "print(\"\\nðŸŽ‰ All VLM models trained!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a9247958",
      "metadata": {
        "id": "a9247958"
      },
      "source": [
        "## ðŸ“Š Step 10: Training Summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "91efdcca",
      "metadata": {
        "id": "91efdcca"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"ðŸŽ‰ ALL TRAINING COMPLETE!\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Compile results\n",
        "all_results = []\n",
        "\n",
        "for model_name, result in llm_results.items():\n",
        "    all_results.append({\n",
        "        'Model': model_name,\n",
        "        'Type': 'LLM',\n",
        "        'Time (hours)': f\"{result['time']/3600:.2f}\"\n",
        "    })\n",
        "\n",
        "for model_name, result in vit_results.items():\n",
        "    all_results.append({\n",
        "        'Model': model_name,\n",
        "        'Type': 'ViT',\n",
        "        'Time (hours)': f\"{result['time']/3600:.2f}\"\n",
        "    })\n",
        "\n",
        "for model_name, result in vlm_results.items():\n",
        "    all_results.append({\n",
        "        'Model': model_name,\n",
        "        'Type': 'VLM',\n",
        "        'Time (hours)': f\"{result['time']/3600:.2f}\"\n",
        "    })\n",
        "\n",
        "df = pd.DataFrame(all_results)\n",
        "print(\"\\nðŸ“Š Training Summary:\")\n",
        "print(df.to_string(index=False))\n",
        "\n",
        "total_time = sum([r['time'] for r in llm_results.values()]) + \\\n",
        "             sum([r['time'] for r in vit_results.values()]) + \\\n",
        "             sum([r['time'] for r in vlm_results.values()])\n",
        "\n",
        "print(f\"\\nâ±ï¸ Total Training Time: {total_time/3600:.2f} hours\")\n",
        "print(f\"\\nðŸ’¾ All checkpoints saved to: {CONFIG['checkpoint_dir']}\")\n",
        "\n",
        "# List all saved checkpoints\n",
        "import os\n",
        "checkpoints = [f for f in os.listdir(CONFIG['checkpoint_dir']) if f.endswith('.pt')]\n",
        "print(f\"\\nðŸ“‚ Checkpoints ({len(checkpoints)} files):\")\n",
        "for ckpt in sorted(checkpoints):\n",
        "    size_mb = os.path.getsize(os.path.join(CONFIG['checkpoint_dir'], ckpt)) / 1e6\n",
        "    print(f\"   â€¢ {ckpt} ({size_mb:.1f} MB)\")\n",
        "\n",
        "print(\"\\nâœ… Ready to generate plots and evaluate!\")\n",
        "print(\"\\nðŸš€ Next steps:\")\n",
        "print(\"   1. Download checkpoints from Google Drive\")\n",
        "print(\"   2. Run evaluation: python publication_plots.py\")\n",
        "print(\"   3. Run comparison: python plot_internet_comparison.py\")\n",
        "print(\"   4. Submit to ICML/NeurIPS 2026!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "93500cd0",
      "metadata": {
        "id": "93500cd0"
      },
      "source": [
        "## ðŸ’¾ Step 11: Download Checkpoints (Optional)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9b33232c",
      "metadata": {
        "id": "9b33232c"
      },
      "outputs": [],
      "source": [
        "# Zip all checkpoints for easy download\n",
        "import shutil\n",
        "\n",
        "print(\"ðŸ“¦ Creating checkpoint archive...\")\n",
        "archive_path = '/content/farmfederate_checkpoints'\n",
        "shutil.make_archive(archive_path, 'zip', CONFIG['checkpoint_dir'])\n",
        "\n",
        "print(f\"âœ… Archive created: {archive_path}.zip\")\n",
        "print(\"\\nðŸ“¥ Download from: Google Drive > FarmFederate_Checkpoints/\")\n",
        "print(\"   Or run: files.download(f'{archive_path}.zip')\")\n",
        "\n",
        "# Uncomment to auto-download (warning: large file!)\n",
        "# from google.colab import files\n",
        "# files.download(f'{archive_path}.zip')"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}