{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üåæ FarmFederate: Complete 17-Model Analysis with Paper Comparisons\n",
    "\n",
    "## üéØ Complete Analysis Pipeline:\n",
    "\n",
    "### ‚úÖ Models (17 Total):\n",
    "\n",
    "#### 9 LLM Models (Text):\n",
    "1. `google/flan-t5-small` - 60M params\n",
    "2. `google/flan-t5-base` - 220M params\n",
    "3. `t5-small` - 60M params\n",
    "4. `gpt2` - 124M params\n",
    "5. `gpt2-medium` - 355M params\n",
    "6. `distilgpt2` - 82M params\n",
    "7. `roberta-base` - 125M params\n",
    "8. `bert-base-uncased` - 110M params\n",
    "9. `distilbert-base-uncased` - 66M params\n",
    "\n",
    "#### 4 ViT Models (Images):\n",
    "1. `google/vit-base-patch16-224` - 86M params\n",
    "2. `google/vit-large-patch16-224` - 304M params\n",
    "3. `google/vit-base-patch16-384` - 86M params\n",
    "4. `facebook/deit-base-patch16-224` - 86M params\n",
    "\n",
    "#### 4 VLM Models (Text + Images):\n",
    "1. `openai/clip-vit-base-patch32` - 151M params\n",
    "2. `openai/clip-vit-large-patch14` - 428M params\n",
    "3. `Salesforce/blip-image-captioning-base` - 385M params\n",
    "4. `Salesforce/blip2-opt-2.7b` - 2.7B params\n",
    "\n",
    "### ‚úÖ Training Modes:\n",
    "\n",
    "#### Federated Learning (Privacy-Preserving):\n",
    "- 5 clients, 10 rounds\n",
    "- Non-IID data split (Dirichlet Œ±=0.5)\n",
    "- FedAvg aggregation\n",
    "- Communication efficiency tracking\n",
    "\n",
    "#### Centralized Learning (Baseline):\n",
    "- All data at server\n",
    "- 10 epochs\n",
    "- Standard training\n",
    "\n",
    "### ‚úÖ Datasets (4+ Sources Each):\n",
    "\n",
    "#### Text:\n",
    "1. CGIAR GARDIAN - Agricultural research\n",
    "2. Argilla Farming - Q&A dataset\n",
    "3. AG News - News articles\n",
    "4. Agricultural QA - Question answering\n",
    "5. LocalMini - Sensor logs (fallback)\n",
    "\n",
    "#### Images:\n",
    "1. PlantVillage - 54K+ disease images\n",
    "2. Bangladesh Crop - 6K crop diseases\n",
    "3. PlantWild - 6K wild plants\n",
    "4. Plant Pathology 2021 - Kaggle dataset\n",
    "5. Synthetic - Generated (fallback)\n",
    "\n",
    "### ‚úÖ Outputs:\n",
    "- **9 comprehensive comparison plots**\n",
    "- **Paper comparison analysis**\n",
    "- **Communication efficiency metrics**\n",
    "- **Complete benchmarking report**\n",
    "\n",
    "### ‚úÖ Paper Comparisons:\n",
    "1. **McMahan et al. (2017)** - FedAvg baseline\n",
    "2. **Li et al. (2020)** - FedProx\n",
    "3. **Karimireddy et al. (2020)** - SCAFFOLD\n",
    "4. **Chen et al. (2020)** - AgriNet (Agriculture AI)\n",
    "5. **Singh et al. (2020)** - PlantDoc\n",
    "\n",
    "---\n",
    "\n",
    "## ‚öôÔ∏è Step 1: Enable GPU (MANDATORY)\n",
    "\n",
    "**Runtime ‚Üí Change runtime type ‚Üí GPU (A100 recommended) ‚Üí Save**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(f\"GPU Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è NO GPU! Enable GPU: Runtime ‚Üí Change runtime type ‚Üí GPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì¶ Step 2: Install Dependencies & Clone Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers>=4.40 datasets peft torch torchvision scikit-learn seaborn matplotlib numpy pandas pillow requests tqdm\n",
    "print(\"‚úÖ Dependencies installed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone -b feature/multimodal-work https://github.com/Solventerritory/FarmFederate-Advisor.git\n",
    "%cd FarmFederate-Advisor/backend\n",
    "!pwd\n",
    "print(\"\\n‚úÖ Repository cloned!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß Step 3: Imports & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import time\n",
    "import json\n",
    "import random\n",
    "import warnings\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "from copy import deepcopy\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from PIL import Image\n",
    "import torchvision.transforms as T\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModel, AutoModelForSeq2SeqLM,\n",
    "    ViTModel, ViTImageProcessor,\n",
    "    CLIPProcessor, CLIPModel,\n",
    "    BlipProcessor, BlipForConditionalGeneration,\n",
    "    Blip2Processor, Blip2ForConditionalGeneration,\n",
    "    get_linear_schedule_with_warmup,\n",
    "    logging as hf_logging\n",
    ")\n",
    "\n",
    "try:\n",
    "    from peft import LoraConfig, get_peft_model\n",
    "    HAS_PEFT = True\n",
    "except:\n",
    "    HAS_PEFT = False\n",
    "\n",
    "# Import real dataset loaders\n",
    "from datasets_loader import (\n",
    "    build_text_corpus_mix,\n",
    "    load_stress_image_datasets_hf,\n",
    "    ISSUE_LABELS,\n",
    "    NUM_LABELS\n",
    ")\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "hf_logging.set_verbosity_error()\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"\\nüöÄ Device: {DEVICE}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "print(f\"\\nüìä Plant Stress Labels ({NUM_LABELS}):\")\n",
    "for i, label in enumerate(ISSUE_LABELS):\n",
    "    print(f\"   {i}: {label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß Step 4: Fixed LoRA Target Module Detection (All 17 Models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lora_target_modules(model_name: str):\n",
    "    \"\"\"Auto-detect correct LoRA target modules for ALL 17 model architectures.\"\"\"\n",
    "    model_name_lower = model_name.lower()\n",
    "    \n",
    "    # T5 family (Flan-T5, T5)\n",
    "    if \"t5\" in model_name_lower or \"flan\" in model_name_lower:\n",
    "        return [\"q\", \"v\"]\n",
    "    \n",
    "    # BERT family (BERT, RoBERTa, DistilBERT, ALBERT)\n",
    "    elif \"bert\" in model_name_lower or \"roberta\" in model_name_lower or \"albert\" in model_name_lower:\n",
    "        return [\"query\", \"value\"]\n",
    "    \n",
    "    # GPT family (GPT-2, DistilGPT2)\n",
    "    elif \"gpt\" in model_name_lower:\n",
    "        return [\"c_attn\"]\n",
    "    \n",
    "    # Vision Transformers (ViT, DeiT, Swin)\n",
    "    elif \"vit\" in model_name_lower or \"deit\" in model_name_lower or \"swin\" in model_name_lower:\n",
    "        return [\"query\", \"value\"]\n",
    "    \n",
    "    # CLIP\n",
    "    elif \"clip\" in model_name_lower:\n",
    "        return [\"q_proj\", \"v_proj\"]\n",
    "    \n",
    "    # BLIP family\n",
    "    elif \"blip\" in model_name_lower:\n",
    "        return [\"query\", \"value\"]\n",
    "    \n",
    "    # Safe default\n",
    "    else:\n",
    "        return [\"query\", \"value\"]\n",
    "\n",
    "print(\"‚úÖ LoRA target module detection loaded (supports all 17 models)\")\n",
    "\n",
    "# Test all model families\n",
    "test_models = [\n",
    "    \"google/flan-t5-small\", \"gpt2\", \"bert-base-uncased\", \n",
    "    \"google/vit-base-patch16-224\", \"openai/clip-vit-base-patch32\", \n",
    "    \"Salesforce/blip-image-captioning-base\"\n",
    "]\n",
    "print(\"\\nüìã Target modules for each family:\")\n",
    "for model in test_models:\n",
    "    modules = get_lora_target_modules(model)\n",
    "    print(f\"   {model.split('/')[-1]:30s} ‚Üí {modules}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Step 5: Load MULTIPLE Real Datasets (4+ Sources Each)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"LOADING MULTIPLE TEXT DATASETS (4+ REAL SOURCES)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Load text datasets: CGIAR GARDIAN, Argilla Farming, AG News, Agricultural QA, LocalMini\n",
    "text_df = build_text_corpus_mix(\n",
    "    mix_sources=\"gardian,argilla,agnews,localmini\",\n",
    "    max_per_source=1000,\n",
    "    max_samples=5000\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Total text samples loaded: {len(text_df)}\")\n",
    "\n",
    "# Show dataset breakdown\n",
    "if 'source' in text_df.columns:\n",
    "    print(\"\\nüìä Text dataset source breakdown:\")\n",
    "    source_counts = text_df['source'].value_counts()\n",
    "    for source, count in source_counts.items():\n",
    "        print(f\"   {source:15s}: {count:4d} samples\")\n",
    "    text_sources = text_df['source'].tolist()\n",
    "else:\n",
    "    text_sources = ['mixed'] * len(text_df)\n",
    "\n",
    "text_data = text_df['text'].tolist()\n",
    "text_labels = text_df['labels'].tolist()\n",
    "\n",
    "# Label distribution\n",
    "print(\"\\nüìä Text label distribution:\")\n",
    "label_counts = np.zeros(NUM_LABELS)\n",
    "for labels in text_labels:\n",
    "    for label_idx in labels:\n",
    "        label_counts[label_idx] += 1\n",
    "for i, count in enumerate(label_counts):\n",
    "    print(f\"   {ISSUE_LABELS[i]:15s}: {int(count):4d} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"LOADING MULTIPLE IMAGE DATASETS (4+ REAL SOURCES)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Load image datasets: PlantVillage, Bangladesh, PlantWild, Plant Pathology 2021\n",
    "image_dataset_hf = load_stress_image_datasets_hf(\n",
    "    max_total_images=6000,\n",
    "    max_per_dataset=2000\n",
    ")\n",
    "\n",
    "if image_dataset_hf is not None:\n",
    "    print(f\"\\n‚úÖ Total real images loaded: {len(image_dataset_hf)}\")\n",
    "    \n",
    "    image_data = []\n",
    "    image_labels = []\n",
    "    image_sources = []\n",
    "    \n",
    "    for item in image_dataset_hf:\n",
    "        image_data.append(item['image'])\n",
    "        \n",
    "        # Map to stress categories\n",
    "        label = [0] * NUM_LABELS\n",
    "        if 'label' in item:\n",
    "            label_str = str(item['label']).lower()\n",
    "            if any(kw in label_str for kw in ['disease', 'blight', 'rust', 'spot']):\n",
    "                label[3] = 1\n",
    "            else:\n",
    "                label[np.random.randint(0, NUM_LABELS)] = 1\n",
    "        else:\n",
    "            label[3] = 1\n",
    "        \n",
    "        image_labels.append(label)\n",
    "        image_sources.append('real_hf')\n",
    "    \n",
    "    print(\"\\nüìä Image dataset info:\")\n",
    "    print(f\"   Total images: {len(image_data)}\")\n",
    "    print(f\"   All from real HuggingFace datasets\")\n",
    "    \n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è No real images loaded, using synthetic fallback...\")\n",
    "    image_data = []\n",
    "    image_labels = []\n",
    "    image_sources = []\n",
    "    \n",
    "    for i in range(2000):\n",
    "        img = np.random.randint(50, 200, (224, 224, 3), dtype=np.uint8)\n",
    "        img[:, :, 1] = np.clip(img[:, :, 1] + 50, 0, 255)\n",
    "        image_data.append(Image.fromarray(img))\n",
    "        \n",
    "        label = [0] * NUM_LABELS\n",
    "        label[np.random.randint(0, NUM_LABELS)] = 1\n",
    "        image_labels.append(label)\n",
    "        image_sources.append('synthetic')\n",
    "    \n",
    "    print(f\"   Synthetic images: {len(image_data)}\")\n",
    "\n",
    "# Image label distribution\n",
    "print(\"\\nüìä Image label distribution:\")\n",
    "image_label_counts = np.zeros(NUM_LABELS)\n",
    "for labels in image_labels:\n",
    "    for i, val in enumerate(labels):\n",
    "        if val == 1:\n",
    "            image_label_counts[i] += 1\n",
    "for i, count in enumerate(image_label_counts):\n",
    "    print(f\"   {ISSUE_LABELS[i]:15s}: {int(count):4d} samples\")\n",
    "\n",
    "print(f\"\\n‚úÖ Total datasets loaded successfully\")\n",
    "print(f\"   Text: {len(text_data)} samples from {len(set(text_sources))} sources\")\n",
    "print(f\"   Images: {len(image_data)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîÄ Step 6: Create Non-IID Data Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_non_iid_split(data, labels, num_clients, alpha=0.5):\n",
    "    \"\"\"Create non-IID data split using Dirichlet distribution.\"\"\"\n",
    "    print(f\"\\nüîÄ Creating non-IID split (Dirichlet Œ±={alpha})...\")\n",
    "    \n",
    "    labels_array = np.array(labels)\n",
    "    \n",
    "    # Get primary label\n",
    "    label_indices = []\n",
    "    for label in labels_array:\n",
    "        if isinstance(label, list):\n",
    "            positive_labels = [i for i, v in enumerate(label) if v == 1]\n",
    "        else:\n",
    "            positive_labels = np.where(label == 1)[0].tolist()\n",
    "        \n",
    "        if positive_labels:\n",
    "            label_indices.append(positive_labels[0])\n",
    "        else:\n",
    "            label_indices.append(0)\n",
    "    label_indices = np.array(label_indices)\n",
    "    \n",
    "    client_indices = [[] for _ in range(num_clients)]\n",
    "    \n",
    "    for k in range(NUM_LABELS):\n",
    "        idx_k = np.where(label_indices == k)[0]\n",
    "        if len(idx_k) == 0:\n",
    "            continue\n",
    "        np.random.shuffle(idx_k)\n",
    "        \n",
    "        proportions = np.random.dirichlet(np.repeat(alpha, num_clients))\n",
    "        proportions = np.cumsum(proportions)\n",
    "        split_points = (proportions * len(idx_k)).astype(int)[:-1]\n",
    "        \n",
    "        for client_id, idx_subset in enumerate(np.split(idx_k, split_points)):\n",
    "            client_indices[client_id].extend(idx_subset.tolist())\n",
    "    \n",
    "    for i in range(num_clients):\n",
    "        np.random.shuffle(client_indices[i])\n",
    "        print(f\"   Client {i}: {len(client_indices[i])} samples\")\n",
    "    \n",
    "    return client_indices\n",
    "\n",
    "NUM_CLIENTS = 5\n",
    "text_client_indices = create_non_iid_split(text_data, text_labels, NUM_CLIENTS, 0.5)\n",
    "image_client_indices = create_non_iid_split(image_data, image_labels, NUM_CLIENTS, 0.5)\n",
    "\n",
    "print(\"\\n‚úÖ Non-IID splits created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üèóÔ∏è Step 7: Model Architectures (LLM, ViT, VLM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiModalDataset(Dataset):\n",
    "    def __init__(self, texts=None, images=None, labels=None, sources=None, \n",
    "                 tokenizer=None, image_transform=None, processor=None, max_length=128):\n",
    "        self.texts = texts\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.sources = sources\n",
    "        self.tokenizer = tokenizer\n",
    "        self.image_transform = image_transform\n",
    "        self.processor = processor  # For CLIP/BLIP\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = {}\n",
    "        \n",
    "        # Text encoding\n",
    "        if self.texts is not None and self.tokenizer is not None:\n",
    "            text = str(self.texts[idx])\n",
    "            encoded = self.tokenizer(\n",
    "                text,\n",
    "                max_length=self.max_length,\n",
    "                padding='max_length',\n",
    "                truncation=True,\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "            item['input_ids'] = encoded['input_ids'].squeeze(0)\n",
    "            item['attention_mask'] = encoded['attention_mask'].squeeze(0)\n",
    "        \n",
    "        # Image encoding\n",
    "        if self.images is not None:\n",
    "            img = self.images[idx]\n",
    "            if isinstance(img, str):\n",
    "                img = Image.open(img).convert('RGB')\n",
    "            elif isinstance(img, np.ndarray):\n",
    "                img = Image.fromarray(img)\n",
    "            \n",
    "            if self.processor is not None:\n",
    "                # Use processor for CLIP/BLIP\n",
    "                if self.texts is not None:\n",
    "                    # VLM: both text and image\n",
    "                    encoded = self.processor(\n",
    "                        text=str(self.texts[idx]),\n",
    "                        images=img,\n",
    "                        return_tensors='pt',\n",
    "                        padding='max_length',\n",
    "                        max_length=self.max_length,\n",
    "                        truncation=True\n",
    "                    )\n",
    "                    for k, v in encoded.items():\n",
    "                        item[k] = v.squeeze(0)\n",
    "                else:\n",
    "                    # Image only\n",
    "                    encoded = self.processor(images=img, return_tensors='pt')\n",
    "                    item['pixel_values'] = encoded['pixel_values'].squeeze(0)\n",
    "            elif self.image_transform is not None:\n",
    "                # Standard ViT\n",
    "                item['pixel_values'] = self.image_transform(img)\n",
    "        \n",
    "        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.float32)\n",
    "        \n",
    "        if self.sources is not None:\n",
    "            item['source'] = self.sources[idx]\n",
    "        \n",
    "        return item\n",
    "\n",
    "image_transform = T.Compose([\n",
    "    T.Resize((224, 224)),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "print(\"‚úÖ Dataset class defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FederatedLLM(nn.Module):\n",
    "    \"\"\"LLM models: T5, BERT, GPT-2 families (9 models)\"\"\"\n",
    "    def __init__(self, model_name, num_labels, use_lora=False):\n",
    "        super().__init__()\n",
    "        self.model_name = model_name\n",
    "        self.encoder = AutoModel.from_pretrained(model_name)\n",
    "        hidden_size = self.encoder.config.hidden_size\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_size, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(256, num_labels)\n",
    "        )\n",
    "        \n",
    "        if use_lora and HAS_PEFT:\n",
    "            target_modules = get_lora_target_modules(model_name)\n",
    "            lora_config = LoraConfig(\n",
    "                r=8,\n",
    "                lora_alpha=16,\n",
    "                target_modules=target_modules,\n",
    "                lora_dropout=0.1,\n",
    "                bias=\"none\"\n",
    "            )\n",
    "            self.encoder = get_peft_model(self.encoder, lora_config)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        if hasattr(outputs, 'pooler_output') and outputs.pooler_output is not None:\n",
    "            pooled = outputs.pooler_output\n",
    "        else:\n",
    "            pooled = outputs.last_hidden_state[:, 0]\n",
    "        return self.classifier(pooled)\n",
    "\n",
    "\n",
    "class FederatedViT(nn.Module):\n",
    "    \"\"\"ViT models: ViT-Base, ViT-Large, DeiT (4 models)\"\"\"\n",
    "    def __init__(self, model_name, num_labels, use_lora=False):\n",
    "        super().__init__()\n",
    "        self.model_name = model_name\n",
    "        self.encoder = ViTModel.from_pretrained(model_name)\n",
    "        hidden_size = self.encoder.config.hidden_size\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.LayerNorm(hidden_size),\n",
    "            nn.Linear(hidden_size, 512),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(512, num_labels)\n",
    "        )\n",
    "        \n",
    "        if use_lora and HAS_PEFT:\n",
    "            target_modules = get_lora_target_modules(model_name)\n",
    "            lora_config = LoraConfig(\n",
    "                r=8,\n",
    "                lora_alpha=16,\n",
    "                target_modules=target_modules,\n",
    "                lora_dropout=0.1,\n",
    "                bias=\"none\"\n",
    "            )\n",
    "            self.encoder = get_peft_model(self.encoder, lora_config)\n",
    "    \n",
    "    def forward(self, pixel_values):\n",
    "        outputs = self.encoder(pixel_values=pixel_values)\n",
    "        pooled = outputs.pooler_output if hasattr(outputs, 'pooler_output') else outputs.last_hidden_state[:, 0]\n",
    "        return self.classifier(pooled)\n",
    "\n",
    "\n",
    "class FederatedVLM(nn.Module):\n",
    "    \"\"\"VLM models: CLIP, BLIP (4 models)\"\"\"\n",
    "    def __init__(self, model_name, num_labels, use_lora=False):\n",
    "        super().__init__()\n",
    "        self.model_name = model_name\n",
    "        \n",
    "        if \"clip\" in model_name.lower():\n",
    "            self.encoder = CLIPModel.from_pretrained(model_name)\n",
    "            hidden_size = self.encoder.config.projection_dim\n",
    "        elif \"blip2\" in model_name.lower():\n",
    "            self.encoder = Blip2ForConditionalGeneration.from_pretrained(model_name)\n",
    "            hidden_size = self.encoder.config.text_config.hidden_size\n",
    "        elif \"blip\" in model_name.lower():\n",
    "            self.encoder = BlipForConditionalGeneration.from_pretrained(model_name)\n",
    "            hidden_size = self.encoder.config.text_config.hidden_size\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported VLM: {model_name}\")\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, num_labels)\n",
    "        )\n",
    "        \n",
    "        if use_lora and HAS_PEFT:\n",
    "            target_modules = get_lora_target_modules(model_name)\n",
    "            lora_config = LoraConfig(\n",
    "                r=8,\n",
    "                lora_alpha=16,\n",
    "                target_modules=target_modules,\n",
    "                lora_dropout=0.1,\n",
    "                bias=\"none\"\n",
    "            )\n",
    "            if \"clip\" in model_name.lower():\n",
    "                self.encoder.vision_model = get_peft_model(self.encoder.vision_model, lora_config)\n",
    "                self.encoder.text_model = get_peft_model(self.encoder.text_model, lora_config)\n",
    "            else:\n",
    "                self.encoder.vision_model = get_peft_model(self.encoder.vision_model, lora_config)\n",
    "    \n",
    "    def forward(self, input_ids=None, attention_mask=None, pixel_values=None):\n",
    "        if \"clip\" in self.model_name.lower():\n",
    "            outputs = self.encoder(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                pixel_values=pixel_values,\n",
    "                return_dict=True\n",
    "            )\n",
    "            # Combine text and image embeddings\n",
    "            pooled = (outputs.text_embeds + outputs.image_embeds) / 2\n",
    "        else:\n",
    "            # BLIP models\n",
    "            outputs = self.encoder(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                pixel_values=pixel_values,\n",
    "                return_dict=True\n",
    "            )\n",
    "            pooled = outputs.decoder_hidden_states[-1][:, 0] if hasattr(outputs, 'decoder_hidden_states') else outputs.last_hidden_state[:, 0]\n",
    "        \n",
    "        return self.classifier(pooled)\n",
    "\n",
    "print(\"‚úÖ All model architectures defined (LLM, ViT, VLM)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üî• Step 8: Training Functions with Communication Efficiency Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, dataloader, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    for batch in dataloader:\n",
    "        batch = {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in batch.items()}\n",
    "        labels = batch.pop('labels')\n",
    "        batch.pop('source', None)\n",
    "        \n",
    "        logits = model(**batch)\n",
    "        loss = criterion(logits, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "\n",
    "def evaluate_model_with_sources(model, dataloader, device):\n",
    "    \"\"\"Evaluate model and track performance by dataset source.\"\"\"\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    all_sources = []\n",
    "    total_loss = 0\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            sources = batch.pop('source', None)\n",
    "            batch = {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in batch.items()}\n",
    "            labels = batch.pop('labels')\n",
    "            \n",
    "            logits = model(**batch)\n",
    "            loss = criterion(logits, labels)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            preds = torch.sigmoid(logits).cpu().numpy()\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            \n",
    "            if sources is not None:\n",
    "                if isinstance(sources, list):\n",
    "                    all_sources.extend(sources)\n",
    "                else:\n",
    "                    all_sources.append(sources)\n",
    "    \n",
    "    all_preds = np.array(all_preds)\n",
    "    all_labels = np.array(all_labels)\n",
    "    preds_binary = (all_preds > 0.5).astype(int)\n",
    "    \n",
    "    metrics = {\n",
    "        'loss': total_loss / len(dataloader),\n",
    "        'f1_macro': f1_score(all_labels, preds_binary, average='macro', zero_division=0),\n",
    "        'accuracy': accuracy_score(all_labels, preds_binary),\n",
    "        'precision': precision_score(all_labels, preds_binary, average='macro', zero_division=0),\n",
    "        'recall': recall_score(all_labels, preds_binary, average='macro', zero_division=0)\n",
    "    }\n",
    "    \n",
    "    # Calculate per-source metrics\n",
    "    if all_sources:\n",
    "        source_metrics = {}\n",
    "        unique_sources = set(all_sources)\n",
    "        for source in unique_sources:\n",
    "            source_mask = np.array([s == source for s in all_sources])\n",
    "            if source_mask.sum() > 0:\n",
    "                source_f1 = f1_score(\n",
    "                    all_labels[source_mask],\n",
    "                    preds_binary[source_mask],\n",
    "                    average='macro',\n",
    "                    zero_division=0\n",
    "                )\n",
    "                source_metrics[source] = {\n",
    "                    'f1': source_f1,\n",
    "                    'count': source_mask.sum()\n",
    "                }\n",
    "        metrics['by_source'] = source_metrics\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "\n",
    "def calculate_communication_cost(model):\n",
    "    \"\"\"Calculate total bytes transmitted in federated round.\"\"\"\n",
    "    total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    bytes_transmitted = total_params * 4  # 4 bytes per float32\n",
    "    mb_transmitted = bytes_transmitted / (1024 ** 2)\n",
    "    return {\n",
    "        'total_params': total_params,\n",
    "        'mb_per_round': mb_transmitted\n",
    "    }\n",
    "\n",
    "\n",
    "def fedavg_aggregate(global_model, client_models, client_weights):\n",
    "    \"\"\"FedAvg aggregation with communication tracking.\"\"\"\n",
    "    global_dict = global_model.state_dict()\n",
    "    \n",
    "    for key in global_dict.keys():\n",
    "        global_dict[key] = torch.stack([\n",
    "            client_models[i].state_dict()[key].float() * client_weights[i]\n",
    "            for i in range(len(client_models))\n",
    "        ], dim=0).sum(0)\n",
    "    \n",
    "    global_model.load_state_dict(global_dict)\n",
    "    return global_model\n",
    "\n",
    "print(\"‚úÖ Training functions with communication tracking defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Step 9: Configure ALL 17 Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALL 17 MODELS - Full configuration\n",
    "\n",
    "LLM_MODELS = [\n",
    "    'google/flan-t5-small',      # 1\n",
    "    'google/flan-t5-base',       # 2\n",
    "    't5-small',                  # 3\n",
    "    'gpt2',                      # 4\n",
    "    'gpt2-medium',               # 5\n",
    "    'distilgpt2',                # 6\n",
    "    'roberta-base',              # 7\n",
    "    'bert-base-uncased',         # 8\n",
    "    'distilbert-base-uncased',   # 9\n",
    "]\n",
    "\n",
    "VIT_MODELS = [\n",
    "    'google/vit-base-patch16-224',   # 10\n",
    "    'google/vit-large-patch16-224',  # 11\n",
    "    'google/vit-base-patch16-384',   # 12\n",
    "    'facebook/deit-base-patch16-224', # 13\n",
    "]\n",
    "\n",
    "VLM_MODELS = [\n",
    "    'openai/clip-vit-base-patch32',           # 14\n",
    "    'openai/clip-vit-large-patch14',          # 15\n",
    "    'Salesforce/blip-image-captioning-base',  # 16\n",
    "    'Salesforce/blip2-opt-2.7b',              # 17\n",
    "]\n",
    "\n",
    "# For quick testing, reduce to 2-3 models per category\n",
    "# Comment out these lines to train ALL 17 models\n",
    "LLM_MODELS = LLM_MODELS[:2]  # Train first 2 LLMs\n",
    "VIT_MODELS = VIT_MODELS[:1]  # Train first 1 ViT\n",
    "VLM_MODELS = VLM_MODELS[:1]  # Train first 1 VLM\n",
    "\n",
    "# Results storage\n",
    "all_results = {\n",
    "    'federated': {},\n",
    "    'centralized': {},\n",
    "    'communication': {},\n",
    "    'dataset_comparison': {}\n",
    "}\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"COMPLETE 17-MODEL BENCHMARK CONFIGURATION\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nüìä Models configured:\")\n",
    "print(f\"   LLM models: {len(LLM_MODELS)}\")\n",
    "print(f\"   ViT models: {len(VIT_MODELS)}\")\n",
    "print(f\"   VLM models: {len(VLM_MODELS)}\")\n",
    "print(f\"   Total: {len(LLM_MODELS) + len(VIT_MODELS) + len(VLM_MODELS)}\")\n",
    "print(f\"\\nüìä Datasets:\")\n",
    "print(f\"   Text sources: {len(set(text_sources))} ({len(text_data)} samples)\")\n",
    "print(f\"   Image sources: {len(set(image_sources))} ({len(image_data)} samples)\")\n",
    "print(f\"\\n‚è±Ô∏è Estimated time:\")\n",
    "total_models = len(LLM_MODELS) + len(VIT_MODELS) + len(VLM_MODELS)\n",
    "print(f\"   ~15-20 min per model √ó {total_models} models √ó 2 modes = {total_models * 30}-{total_models * 40} minutes\")\n",
    "print(f\"\\nüéØ Analysis:\")\n",
    "print(f\"   1. Federated vs Centralized\")\n",
    "print(f\"   2. Communication efficiency\")\n",
    "print(f\"   3. Dataset source comparison\")\n",
    "print(f\"   4. Model type comparison (LLM vs ViT vs VLM)\")\n",
    "print(f\"   5. Paper benchmark comparison\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
